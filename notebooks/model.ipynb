{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6f6f4a",
   "metadata": {},
   "source": [
    "# Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e39adef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from tempfile import TemporaryDirectory\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8ddfb",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58200b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pl.read_parquet('../data/processed/output.parquet')\n",
    "train_df = pl.read_parquet('../data/processed/train.parquet')\n",
    "test_df = pl.read_parquet('../data/processed/test.parquet')\n",
    "vocabs = torch.load('../data/processed/all_vocabs.pth')\n",
    "\n",
    "user_vocab = vocabs[\"user_vocab\"]\n",
    "movie_vocab = vocabs[\"movie_vocab\"]\n",
    "genres_vocab = vocabs[\"genres_vocab\"]\n",
    "prod_comp_vocab = vocabs[\"prod_comp_vocab\"]\n",
    "prod_countries_vocab = vocabs[\"prod_countries_vocab\"]\n",
    "languages_vocab = vocabs[\"languages_vocab\"]\n",
    "words_vocab = vocabs[\"words_vocab\"]\n",
    "\n",
    "vocabs = {\n",
    "    \"user_vocab\": user_vocab,\n",
    "    \"movie_vocab\": movie_vocab,\n",
    "    \"genres_vocab\": genres_vocab,\n",
    "    \"prod_comp_vocab\": prod_comp_vocab,\n",
    "    \"prod_countries_vocab\": prod_countries_vocab,\n",
    "    \"languages_vocab\": languages_vocab,\n",
    "    \"words_vocab\": words_vocab,\n",
    "}\n",
    "\n",
    "movie_vocab_stoi = movie_vocab.get_stoi()\n",
    "user_vocab_stoi = user_vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9460c261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies DF shape: (86493, 15)\n",
      "Train DF shape: (100000, 3)\n",
      "Test DF shape: (1000, 3)\n",
      "Vocab sizes: {'user_vocab': 200949, 'movie_vocab': 86494, 'genres_vocab': 21, 'prod_comp_vocab': 45546, 'prod_countries_vocab': 201, 'languages_vocab': 164, 'words_vocab': 270246}\n",
      "\n",
      "First movie row: {'movieId_idx': [29614], 'genres_idx': [[9, 12, 1]], 'production_companies_idx': [[5787, 10790, 33238]], 'production_countries_idx': [[199, 179]], 'spoken_languages_idx': [[6, 23, 137, 47]], 'keywords_idx': [[16308, 175809, 93150, 146205, 199438, 206752, 33269, 147659, 198442, 112495, 215262, 104288, 10122, 84395, 101318, 37337, 35868, 263713, 52720, 82866]], 'overview_idx': [[176485, 24219, 129031, 130595, 204009, 102295, 187482, 46191, 265111, 75891, 120691, 82866, 7607, 80603, 210357, 60107, 248412, 24219, 171073, 185742, 207323, 80603, 208294, 186120, 230514, 250658, 48946, 24219, 162914, 55906, 185742, 145939, 86280, 230898, 120691, 50712, 7607, 233356, 69050, 257026, 247940, 24219, 99177, 12348]], 'tagline_idx': [[214081, 63727, 60107, 120691, 122910, 7607, 120691, 55301]], 'adult_idx': [0], 'vote_average': [8.364], 'vote_count': [34495], 'revenue': [8255.327640000001], 'runtime': [148], 'budget': [1600.0000000000002], 'popularity': [83.952]}\n",
      "First sequence row: {'userId': ['user_20007'], 'sequence_movie_ids': [['movie_64983', 'movie_106916', 'movie_4452', 'movie_41573', 'movie_108932', 'movie_36527', 'movie_79185', 'movie_70599', 'movie_46967', 'movie_43560', 'movie_102903', 'movie_68793']], 'sequence_ratings': [[2.5, 4.0, 3.0, 3.5, 3.0, 4.0, 2.5, 3.0, 4.0, 3.5, 3.0, 4.5]]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Movies DF shape: {movies_df.shape}\")\n",
    "print(f\"Train DF shape: {train_df.shape}\")\n",
    "print(f\"Test DF shape: {test_df.shape}\")\n",
    "print(\"Vocab sizes:\", {k: len(v) for k, v in vocabs.items()})\n",
    "print(\"\\nFirst movie row:\", movies_df[0].to_dict(as_series=False))\n",
    "print(\"First sequence row:\", train_df[0].to_dict(as_series=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55ad7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get Vocab Sizes for Model ---\n",
    "len_genres = len(genres_vocab)\n",
    "len_prod_comp = len(prod_comp_vocab)\n",
    "len_prod_cont = len(prod_countries_vocab)\n",
    "len_langs = len(languages_vocab)\n",
    "len_words = len(words_vocab)\n",
    "n_items = len(movie_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "180a2eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class MovieSeqDataset(Dataset):\n",
    "    def __init__(self, data, movie_vocab_stoi, user_vocab_stoi):\n",
    "        self.data = data\n",
    "        self.movie_vocab_stoi = movie_vocab_stoi\n",
    "        self.user_vocab_stoi = user_vocab_stoi\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user, movie_sequence, rating_sequence = self.data[idx]\n",
    "        movie_data = [self.movie_vocab_stoi.get(item,movie_vocab_stoi['<unk>']) for item in movie_sequence.to_list()[0]]\n",
    "        user_data = self.user_vocab_stoi[user.to_list()[0]]\n",
    "        rating_sequence = rating_sequence.to_list()[0]\n",
    "        return torch.tensor(movie_data[:-1],device='cuda'), torch.tensor(user_data), torch.tensor(rating_sequence[:-1]), torch.tensor(movie_data[-1],device='cuda')\n",
    "\n",
    "def collate_batch(batch):\n",
    "    movie_list = [item[0] for item in batch]\n",
    "    user_list = [item[1] for item in batch]\n",
    "    rating_list = [item[2] for item in batch]\n",
    "    target_list = [item[3] for item in batch]\n",
    "    return pad_sequence(movie_list, padding_value=movie_vocab_stoi['<unk>'], batch_first=True), torch.stack(user_list), pad_sequence(rating_list, padding_value=3, batch_first=True), torch.stack(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a64a911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "train_dataset = MovieSeqDataset(train_df, movie_vocab_stoi, user_vocab_stoi)\n",
    "val_dataset = MovieSeqDataset(test_df, movie_vocab_stoi, user_vocab_stoi)\n",
    "\n",
    "train_iter = DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True, collate_fn=collate_batch)\n",
    "val_iter = DataLoader(val_dataset, batch_size=BATCH_SIZE,shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f928e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 11]) torch.Size([256]) torch.Size([256, 11]) torch.Size([256])\n",
      "tensor([86410, 18928, 26366, 29551,  6660, 41632, 26712,  9479, 56031, 68298,\n",
      "        29601], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i, (movie_data, user_data, ratings_data, y_train) in enumerate(train_iter):\n",
    "    print(movie_data.shape, user_data.shape, ratings_data.shape, y_train.shape)\n",
    "    print(movie_data[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60dbc3a",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3c03ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from typing import Tuple\n",
    "\n",
    "class MovieEmbeddings(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 hidden_size: int,\n",
    "                 num_list_features: int,\n",
    "                 num_scalar_features: int,\n",
    "                 n_genres: int, \n",
    "                 n_production_companies: int,\n",
    "                 n_production_countries: int,\n",
    "                 n_spoken_languages: int,\n",
    "                 n_words: int):\n",
    "        super().__init__()\n",
    "        self.genres_embedding = nn.EmbeddingBag(n_genres, d_model*2, mode='mean')\n",
    "        self.prod_comp_embedding = nn.EmbeddingBag(n_production_companies, d_model, mode='mean')\n",
    "        self.prod_cont_embedding = nn.EmbeddingBag(n_production_countries, d_model, mode='mean')\n",
    "        self.lang_embedding = nn.EmbeddingBag(n_spoken_languages, d_model, mode='mean')\n",
    "        self.word_embedding = nn.EmbeddingBag(n_words, d_model*4, mode='mean')\n",
    "        self.fc = nn.Linear(d_model*(10+num_list_features)+num_scalar_features,hidden_size)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.xavier_uniform_(self.genres_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.prod_comp_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.prod_cont_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.lang_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.word_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def _prepare_embedding_inputs(self, list_of_lists) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        flat_list = []\n",
    "        offsets = [0]\n",
    "        for sublist in list_of_lists:\n",
    "            flat_list.extend(sublist)\n",
    "            offsets.append(offsets[-1] + len(sublist))\n",
    "        offsets = offsets[:-1]  # Remove last cumulative sum\n",
    "        offsets = torch.tensor(offsets, dtype=torch.long,device='cuda')\n",
    "        flat_list = torch.tensor(flat_list, dtype=torch.long,device='cuda')\n",
    "        return flat_list, offsets   \n",
    "\n",
    "    def forward(self, row: pl.DataFrame) -> torch.Tensor:\n",
    "        genres, genres_offsets = self._prepare_embedding_inputs(row['genres_idx'])\n",
    "        genres_e = self.genres_embedding(genres, genres_offsets)\n",
    "\n",
    "        comp, comp_offsets = self._prepare_embedding_inputs(row['production_companies_idx'])\n",
    "        comp_e = self.prod_comp_embedding(comp, comp_offsets)\n",
    "\n",
    "        cont, cont_offsets = self._prepare_embedding_inputs(row['production_countries_idx'])\n",
    "        cont_e = self.prod_cont_embedding(cont, cont_offsets)\n",
    "\n",
    "        lang, lang_offsets = self._prepare_embedding_inputs(row['spoken_languages_idx'])\n",
    "        lang_e = self.lang_embedding(lang, lang_offsets)\n",
    "\n",
    "        kw, kw_offsets = self._prepare_embedding_inputs(row['keywords_idx'])\n",
    "        kw_e = self.word_embedding(kw, kw_offsets)\n",
    "\n",
    "        tag, tag_offsets = self._prepare_embedding_inputs(row['tagline_idx'])\n",
    "        tag_e = self.word_embedding(tag, tag_offsets)\n",
    "\n",
    "        ov, ov_offsets = self._prepare_embedding_inputs(row['overview_idx'])\n",
    "        ov_e = self.word_embedding(ov, ov_offsets)\n",
    "\n",
    "        # Scalar features as tensors (ensure shape is [batch_size, 1])\n",
    "        revenue = torch.tensor(row[\"revenue\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "        budget = torch.tensor(row[\"budget\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "        runtime = torch.tensor(row[\"runtime\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "        adult_idx = torch.tensor(row[\"adult_idx\"], dtype=torch.bool,device='cuda').unsqueeze(1)\n",
    "        vote_average = torch.tensor(row[\"vote_average\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "        vote_count = torch.tensor(row[\"vote_count\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "        popularity = torch.tensor(row[\"popularity\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "\n",
    "        # Concatenate all embeddings and scalar features\n",
    "        master_embedding = torch.cat([\n",
    "            genres_e,\n",
    "            comp_e,\n",
    "            cont_e,\n",
    "            lang_e,\n",
    "            kw_e,\n",
    "            tag_e,\n",
    "            ov_e,\n",
    "            revenue,\n",
    "            budget,\n",
    "            runtime,\n",
    "            adult_idx,\n",
    "            vote_average,\n",
    "            vote_count,\n",
    "            popularity\n",
    "        ], dim=1)\n",
    "\n",
    "        return self.fc(master_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df8d30f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e06f6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRXTransformer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, batch_first=True) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x)\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6532702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRXModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 n_heads: int,\n",
    "                 num_layers: int,\n",
    "                 hidden_size: int,\n",
    "                 num_list_features: int,\n",
    "                 num_scalar_features: int,\n",
    "                 n_genres: int, \n",
    "                 n_production_companies: int,\n",
    "                 n_production_countries: int,\n",
    "                 n_spoken_languages: int,\n",
    "                 movie_vocab_stoi: Dict[str, int],\n",
    "                 user_vocab_stoi: Dict[str, int],\n",
    "                 n_movies: int,\n",
    "                 n_words: int,\n",
    "                 seq_len: int = 11,\n",
    "                 dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.movie_embeddings = MovieEmbeddings(d_model, hidden_size, num_list_features, num_scalar_features, n_genres, n_production_companies, n_production_countries, n_spoken_languages, n_words)\n",
    "        self.movie_vocab_stoi = movie_vocab_stoi\n",
    "        self.user_vocab_stoi = user_vocab_stoi\n",
    "        self.transformer_encoder = TRXTransformer(hidden_size, n_heads=n_heads, num_layers=num_layers)\n",
    "        self.fc1 = nn.Linear(hidden_size*seq_len, n_movies)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # # rows['sequence_movie_ids'] -> List[List[Any]] of shape [batch_size, 4]\n",
    "        # batch_movie_ids = rows['sequence_movie_ids'].to_list()\n",
    "\n",
    "        # # Map movie IDs to vocab indices using movie_vocab_stoi\n",
    "        # indexed_ids = [\n",
    "        #     [movie_vocab_stoi.get(str(mid), movie_vocab_stoi['<unk>']) for mid in seq]\n",
    "        #     for seq in batch_movie_ids\n",
    "        # ]\n",
    "        \n",
    "        # Fetch the rows filtered by the movie IDs\n",
    "        movie_rows = [movies_df.filter(pl.col('movieId_idx').is_in(x[i])) for i in range(len(x))] # List[pl.DataFrame]\n",
    "\n",
    "        # Get embeddings: [batch_size, seq_len, embedding_dim]\n",
    "        embeddings = [self.movie_embeddings(row) for row in movie_rows]\n",
    "\n",
    "        # stack embeddings if all rows are same size otherwise pad\n",
    "        max_len = max([emb.shape[0] for emb in embeddings])\n",
    "        for i in range(len(embeddings)):\n",
    "            if embeddings[i].shape[0] < max_len:\n",
    "                pad = torch.zeros(max_len - embeddings[i].shape[0], embeddings[i].shape[1], device='cuda')\n",
    "                embeddings[i] = torch.cat([embeddings[i], pad], dim=0)\n",
    "            else:\n",
    "                embeddings[i] = embeddings[i][:max_len]\n",
    "        embeddings = torch.stack(embeddings, dim=0)\n",
    "        # embeddings shape: [batch_size, seq_len, d_model]\n",
    "\n",
    "\n",
    "        # Pass Embeddings through the Transformer Encoder\n",
    "        transformer_output = self.transformer_encoder(embeddings)\n",
    "        # transformer_output shape: [batch_size, seq_len, d_model]\n",
    "\n",
    "        # Reshape transformer_output to [batch_size, seq_len * d_model]\n",
    "        transformer_output = transformer_output.view(transformer_output.size(0), -1)\n",
    "        # transformer_output shape: [batch_size, seq_len * d_model]\n",
    "        # print(f\"Transformer output shape: {transformer_output.shape}\")\n",
    "\n",
    "        # Pass to fc1\n",
    "        fc1_output = self.fc1(transformer_output)\n",
    "        # fc1_output shape: [batch_size, n_movies]\n",
    "\n",
    "        # Apply dropout\n",
    "        fc1_output = self.dropout(fc1_output)\n",
    "        # fc1_output shape: [batch_size, n_movies]\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = F.softmax(fc1_output, dim=1)\n",
    "        # probabilities shape: [batch_size, n_movies]\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d0235f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 11]) torch.Size([256]) torch.Size([256, 11]) torch.Size([256])\n",
      "tensor([58010, 59048, 63599,    95, 10340, 66297, 29770, 13485, 21677, 15808,\n",
      "        21991], device='cuda:0')\n",
      "torch.Size([256, 86494])\n"
     ]
    }
   ],
   "source": [
    "model = TRXModel(\n",
    "    d_model=16,\n",
    "    n_heads=4,\n",
    "    num_layers=4,\n",
    "    hidden_size=256,\n",
    "    num_list_features=7,\n",
    "    num_scalar_features=7,\n",
    "    n_genres=len_genres, \n",
    "    n_production_companies=len_prod_comp, \n",
    "    n_production_countries=len_prod_cont, \n",
    "    n_spoken_languages=len_langs, \n",
    "    movie_vocab_stoi=movie_vocab_stoi,\n",
    "    user_vocab_stoi=user_vocab_stoi,\n",
    "    n_movies=len(movie_vocab_stoi),\n",
    "    n_words=len_words\n",
    ").to('cuda')\n",
    "\n",
    "for idx, (movie_data, user_data, ratings_data, y_train) in enumerate(train_iter):\n",
    "    print(movie_data.shape, user_data.shape, ratings_data.shape, y_train.shape)\n",
    "    print(movie_data[0])\n",
    "    y = model(movie_data)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c8a2add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 267.02M\n"
     ]
    }
   ],
   "source": [
    "# Total parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters in the model: {total_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRXModel(\n",
       "  (movie_embeddings): MovieEmbeddings(\n",
       "    (genres_embedding): EmbeddingBag(21, 32, mode='mean')\n",
       "    (prod_comp_embedding): EmbeddingBag(45546, 16, mode='mean')\n",
       "    (prod_cont_embedding): EmbeddingBag(201, 16, mode='mean')\n",
       "    (lang_embedding): EmbeddingBag(164, 16, mode='mean')\n",
       "    (word_embedding): EmbeddingBag(270246, 64, mode='mean')\n",
       "    (fc): Linear(in_features=279, out_features=256, bias=True)\n",
       "  )\n",
       "  (transformer_encoder): TRXTransformer(\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=2816, out_features=86494, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 5\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TRXModel(\n",
    "    d_model=16,\n",
    "    n_heads=4,\n",
    "    num_layers=4,\n",
    "    hidden_size=256,\n",
    "    num_list_features=7,\n",
    "    num_scalar_features=7,\n",
    "    n_genres=len_genres, \n",
    "    n_production_companies=len_prod_comp, \n",
    "    n_production_countries=len_prod_cont, \n",
    "    n_spoken_languages=len_langs, \n",
    "    movie_vocab_stoi=movie_vocab_stoi,\n",
    "    user_vocab_stoi=user_vocab_stoi,\n",
    "    n_movies=len(movie_vocab_stoi),\n",
    "    n_words=len_words\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model: nn.Module, train_iter, epoch) -> None:\n",
    "    # Switch to training mode\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, (movie_data, _, _, target) in tqdm(enumerate(train_iter)):\n",
    "        # Load movie sequence and user id\n",
    "        movie_data = movie_data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Predict movies\n",
    "        output = model(movie_data) # [batch-size, n_movies]\n",
    "        \n",
    "        # Backpropogation process\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        # Results\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8305490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_data: torch.Tensor) -> float:\n",
    "    # Switch the model to evaluation mode.\n",
    "    # This is necessary for layers like dropout,\n",
    "    model.eval() \n",
    "    total_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (movie_data, _, _, target) in tqdm(enumerate(eval_data)):\n",
    "            # Load movie sequence and user id\n",
    "            movie_data = movie_data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            # Predict movies\n",
    "            output = model(movie_data)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [02:15,  3.97s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[0;32m     11\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_iter)\n",
      "Cell \u001b[1;32mIn[24], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_iter, epoch)\u001b[0m\n\u001b[0;32m     13\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Predict movies\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [batch-size, n_movies]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Backpropogation process\u001b[39;00m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n",
      "File \u001b[1;32mc:\\Users\\arnav\\Documents\\VSCodeProjects\\RecSysTRX\\RecSysTRX\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\arnav\\Documents\\VSCodeProjects\\RecSysTRX\\RecSysTRX\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 41\u001b[0m, in \u001b[0;36mTRXModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m movie_rows \u001b[38;5;241m=\u001b[39m [movies_df\u001b[38;5;241m.\u001b[39mfilter(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovieId_idx\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mis_in(x[i])) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x))] \u001b[38;5;66;03m# List[pl.DataFrame]\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Get embeddings: [batch_size, seq_len, embedding_dim]\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmovie_embeddings(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m movie_rows]\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# stack embeddings if all rows are same size otherwise pad\u001b[39;00m\n\u001b[0;32m     44\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([emb\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m embeddings])\n",
      "Cell \u001b[1;32mIn[20], line 41\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     38\u001b[0m movie_rows \u001b[38;5;241m=\u001b[39m [movies_df\u001b[38;5;241m.\u001b[39mfilter(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovieId_idx\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mis_in(x[i])) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x))] \u001b[38;5;66;03m# List[pl.DataFrame]\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Get embeddings: [batch_size, seq_len, embedding_dim]\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmovie_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m movie_rows]\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# stack embeddings if all rows are same size otherwise pad\u001b[39;00m\n\u001b[0;32m     44\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([emb\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m embeddings])\n",
      "File \u001b[1;32mc:\\Users\\arnav\\Documents\\VSCodeProjects\\RecSysTRX\\RecSysTRX\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\arnav\\Documents\\VSCodeProjects\\RecSysTRX\\RecSysTRX\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 58\u001b[0m, in \u001b[0;36mMovieEmbeddings.forward\u001b[1;34m(self, row)\u001b[0m\n\u001b[0;32m     55\u001b[0m lang, lang_offsets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_embedding_inputs(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspoken_languages_idx\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     56\u001b[0m lang_e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang_embedding(lang, lang_offsets)\n\u001b[1;32m---> 58\u001b[0m kw, kw_offsets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_embedding_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeywords_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m kw_e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embedding(kw, kw_offsets)\n\u001b[0;32m     61\u001b[0m tag, tag_offsets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_embedding_inputs(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtagline_idx\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[16], line 37\u001b[0m, in \u001b[0;36mMovieEmbeddings._prepare_embedding_inputs\u001b[1;34m(self, list_of_lists)\u001b[0m\n\u001b[0;32m     35\u001b[0m flat_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     36\u001b[0m offsets \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m list_of_lists:\n\u001b[0;32m     38\u001b[0m     flat_list\u001b[38;5;241m.\u001b[39mextend(sublist)\n\u001b[0;32m     39\u001b[0m     offsets\u001b[38;5;241m.\u001b[39mappend(offsets[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(sublist))\n",
      "File \u001b[1;32mc:\\Users\\arnav\\Documents\\VSCodeProjects\\RecSysTRX\\RecSysTRX\\.venv\\lib\\site-packages\\polars\\series\\series.py:1283\u001b[0m, in \u001b[0;36mSeries.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Generator[Any]:\n\u001b[1;32m-> 1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m \u001b[38;5;129;01min\u001b[39;00m (List, Array):\n\u001b[0;32m   1284\u001b[0m         \u001b[38;5;66;03m# TODO: either make a change and return py-native list data here, or find\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m         \u001b[38;5;66;03m#  a faster way to return nested/List series; sequential 'get_index' calls\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m         \u001b[38;5;66;03m#  make this path a lot slower (~10x) than it needs to be.\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m         get_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_s\u001b[38;5;241m.\u001b[39mget_index\n\u001b[0;32m   1288\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlen()):\n",
      "File \u001b[1;32mc:\\Users\\arnav\\Documents\\VSCodeProjects\\RecSysTRX\\RecSysTRX\\.venv\\lib\\site-packages\\polars\\series\\series.py:596\u001b[0m, in \u001b[0;36mSeries.dtype\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataType:\n\u001b[0;32m    587\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;124;03m    Get the data type of this Series.\u001b[39;00m\n\u001b[0;32m    589\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;124;03m    Int64\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arnav\\Documents\\VSCodeProjects\\RecSysTRX\\RecSysTRX\\.venv\\lib\\site-packages\\polars\\datatypes\\classes.py:774\u001b[0m, in \u001b[0;36mList.__init__\u001b[1;34m(self, inner)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner: PolarsDataType \u001b[38;5;241m|\u001b[39m PythonDataType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner \u001b[38;5;241m=\u001b[39m \u001b[43mpolars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatatypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_into_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arnav\\Documents\\VSCodeProjects\\RecSysTRX\\RecSysTRX\\.venv\\lib\\site-packages\\polars\\datatypes\\_parse.py:53\u001b[0m, in \u001b[0;36mparse_into_dtype\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse_into_dtype\u001b[39m(\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PolarsDataType:\n\u001b[0;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m    Parse an input into a Polars data type.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m        If the input cannot be parsed into a Polars data type.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_polars_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, ForwardRef):\n",
      "File \u001b[1;32mc:\\Users\\arnav\\Documents\\VSCodeProjects\\RecSysTRX\\RecSysTRX\\.venv\\lib\\site-packages\\polars\\datatypes\\convert.py:76\u001b[0m, in \u001b[0;36mis_polars_dtype\u001b[1;34m(dtype, include_unknown, require_instantiated)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Indicate whether the given input is a Polars dtype, or dtype specialization.\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m check_classes \u001b[38;5;241m=\u001b[39m DataType \u001b[38;5;28;01mif\u001b[39;00m require_instantiated \u001b[38;5;28;01melse\u001b[39;00m (DataType, DataTypeClass)\n\u001b[1;32m---> 76\u001b[0m is_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m include_unknown:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m is_dtype \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;241m!=\u001b[39m Unknown\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Training\n",
    "        train(model, train_iter, epoch)\n",
    "\n",
    "        # Evaluation\n",
    "        val_loss = evaluate(model, val_iter)\n",
    "\n",
    "        # Compute the perplexity of the validation loss\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "\n",
    "        # Results\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
