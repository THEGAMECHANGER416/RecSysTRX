{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6f6f4a",
   "metadata": {},
   "source": [
    "# Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e39adef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from tempfile import TemporaryDirectory\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8ddfb",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58200b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tuffy\\Documents\\RecSysProject\\RecSysTRX\\.venv\\lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\Tuffy\\Documents\\RecSysProject\\RecSysTRX\\.venv\\lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "movies_df = pl.read_parquet('../data/processed/output.parquet')\n",
    "train_df = pl.read_parquet('../data/processed/train.parquet')\n",
    "test_df = pl.read_parquet('../data/processed/test.parquet')\n",
    "vocabs = torch.load('../data/processed/all_vocabs.pth')\n",
    "\n",
    "user_vocab = vocabs[\"user_vocab\"]\n",
    "movie_vocab = vocabs[\"movie_vocab\"]\n",
    "genres_vocab = vocabs[\"genres_vocab\"]\n",
    "prod_comp_vocab = vocabs[\"prod_comp_vocab\"]\n",
    "prod_countries_vocab = vocabs[\"prod_countries_vocab\"]\n",
    "languages_vocab = vocabs[\"languages_vocab\"]\n",
    "words_vocab = vocabs[\"words_vocab\"]\n",
    "\n",
    "vocabs = {\n",
    "    \"user_vocab\": user_vocab,\n",
    "    \"movie_vocab\": movie_vocab,\n",
    "    \"genres_vocab\": genres_vocab,\n",
    "    \"prod_comp_vocab\": prod_comp_vocab,\n",
    "    \"prod_countries_vocab\": prod_countries_vocab,\n",
    "    \"languages_vocab\": languages_vocab,\n",
    "    \"words_vocab\": words_vocab,\n",
    "}\n",
    "\n",
    "movie_vocab_stoi = movie_vocab.get_stoi()\n",
    "user_vocab_stoi = user_vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9460c261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies DF shape: (86493, 15)\n",
      "Train DF shape: (13218442, 3)\n",
      "Test DF shape: (2330541, 3)\n",
      "Vocab sizes: {'user_vocab': 200949, 'movie_vocab': 86494, 'genres_vocab': 21, 'prod_comp_vocab': 45546, 'prod_countries_vocab': 201, 'languages_vocab': 164, 'words_vocab': 270246}\n",
      "\n",
      "First movie row: {'movieId_idx': [29614], 'genres_idx': [[9, 12, 1]], 'production_companies_idx': [[5787, 10790, 33238]], 'production_countries_idx': [[199, 179]], 'spoken_languages_idx': [[6, 23, 137, 47]], 'keywords_idx': [[16308, 175809, 93150, 146205, 199438, 206752, 33269, 147659, 198442, 112495, 215262, 104288, 10122, 84395, 101318, 37337, 35868, 263713, 52720, 82866]], 'overview_idx': [[176485, 24219, 129031, 130595, 204009, 102295, 187482, 46191, 265111, 75891, 120691, 82866, 7607, 80603, 210357, 60107, 248412, 24219, 171073, 185742, 207323, 80603, 208294, 186120, 230514, 250658, 48946, 24219, 162914, 55906, 185742, 145939, 86280, 230898, 120691, 50712, 7607, 233356, 69050, 257026, 247940, 24219, 99177, 12348]], 'tagline_idx': [[214081, 63727, 60107, 120691, 122910, 7607, 120691, 55301]], 'adult_idx': [0], 'vote_average': [8.364], 'vote_count': [34495], 'revenue': [8255.327640000001], 'runtime': [148], 'budget': [1600.0000000000002], 'popularity': [83.952]}\n",
      "First sequence row: {'userId': ['user_1'], 'sequence_movie_ids': [['movie_2966', 'movie_2890', 'movie_3078', 'movie_2882', 'movie_541']], 'sequence_ratings': [[1.0, 4.0, 2.0, 1.0, 5.0]]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Movies DF shape: {movies_df.shape}\")\n",
    "print(f\"Train DF shape: {train_df.shape}\")\n",
    "print(f\"Test DF shape: {test_df.shape}\")\n",
    "print(\"Vocab sizes:\", {k: len(v) for k, v in vocabs.items()})\n",
    "print(\"\\nFirst movie row:\", movies_df[0].to_dict(as_series=False))\n",
    "print(\"First sequence row:\", train_df[0].to_dict(as_series=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55ad7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get Vocab Sizes for Model ---\n",
    "len_genres = len(genres_vocab)\n",
    "len_prod_comp = len(prod_comp_vocab)\n",
    "len_prod_cont = len(prod_countries_vocab)\n",
    "len_langs = len(languages_vocab)\n",
    "len_words = len(words_vocab)\n",
    "n_items = len(movie_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "180a2eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class MovieSeqDataset(Dataset):\n",
    "    def __init__(self, data, movie_vocab_stoi, user_vocab_stoi):\n",
    "        self.data = data\n",
    "        self.movie_vocab_stoi = movie_vocab_stoi\n",
    "        self.user_vocab_stoi = user_vocab_stoi\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user, movie_sequence, rating_sequence = self.data[idx]\n",
    "        movie_data = [self.movie_vocab_stoi.get(item,movie_vocab_stoi['<unk>']) for item in movie_sequence.to_list()[0]]\n",
    "        user_data = self.user_vocab_stoi[user.to_list()[0]]\n",
    "        rating_sequence = rating_sequence.to_list()[0]\n",
    "        return torch.tensor(movie_data[:-1]), torch.tensor(user_data), torch.tensor(rating_sequence[:-1]), torch.tensor(movie_data[-1])\n",
    "    \n",
    "def collate_batch(batch):\n",
    "    movie_list = [item[0] for item in batch]\n",
    "    user_list = [item[1] for item in batch]\n",
    "    rating_list = [item[2] for item in batch]\n",
    "    target_list = [item[3] for item in batch]\n",
    "    return pad_sequence(movie_list, padding_value=movie_vocab_stoi['<unk>'], batch_first=True), torch.stack(user_list), pad_sequence(rating_list, padding_value=3, batch_first=True), torch.stack(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a64a911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = MovieSeqDataset(train_df, movie_vocab_stoi, user_vocab_stoi)\n",
    "val_dataset = MovieSeqDataset(test_df, movie_vocab_stoi, user_vocab_stoi)\n",
    "\n",
    "train_iter = DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True, collate_fn=collate_batch)\n",
    "val_iter = DataLoader(val_dataset, batch_size=BATCH_SIZE,shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5f928e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4]) torch.Size([16]) torch.Size([16, 4]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for i, (movie_data, user_data, ratings_data, y_train) in enumerate(train_iter):\n",
    "    print(movie_data.shape, user_data.shape, ratings_data.shape, y_train.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60dbc3a",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3c03ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from typing import Tuple\n",
    "\n",
    "class MovieEmbeddings(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 hidden_size: int,\n",
    "                 num_list_features: int,\n",
    "                 num_scalar_features: int,\n",
    "                 n_genres: int, \n",
    "                 n_production_companies: int,\n",
    "                 n_production_countries: int,\n",
    "                 n_spoken_languages: int,\n",
    "                 n_words: int):\n",
    "        super().__init__()\n",
    "        self.genres_embedding = nn.EmbeddingBag(n_genres, d_model*2, mode='mean')\n",
    "        self.prod_comp_embedding = nn.EmbeddingBag(n_production_companies, d_model, mode='mean')\n",
    "        self.prod_cont_embedding = nn.EmbeddingBag(n_production_countries, d_model, mode='mean')\n",
    "        self.lang_embedding = nn.EmbeddingBag(n_spoken_languages, d_model, mode='mean')\n",
    "        self.word_embedding = nn.EmbeddingBag(n_words, d_model*4, mode='mean')\n",
    "        self.fc = nn.Linear(d_model*(10+num_list_features)+num_scalar_features,hidden_size)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.xavier_uniform_(self.genres_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.prod_comp_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.prod_cont_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.lang_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.word_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def _prepare_embedding_inputs(self, list_of_lists) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        flat_list = []\n",
    "        offsets = [0]\n",
    "        for sublist in list_of_lists:\n",
    "            flat_list.extend(sublist)\n",
    "            offsets.append(offsets[-1] + len(sublist))\n",
    "        offsets = offsets[:-1]  # Remove last cumulative sum\n",
    "        offsets = torch.tensor(offsets, dtype=torch.long)\n",
    "        flat_list = torch.tensor(flat_list, dtype=torch.long)\n",
    "        return flat_list, offsets   \n",
    "\n",
    "    def forward(self, row: pl.DataFrame) -> torch.Tensor:\n",
    "        genres, genres_offsets = self._prepare_embedding_inputs(row['genres_idx'])\n",
    "        genres_e = self.genres_embedding(genres, genres_offsets)\n",
    "\n",
    "        comp, comp_offsets = self._prepare_embedding_inputs(row['production_companies_idx'])\n",
    "        comp_e = self.prod_comp_embedding(comp, comp_offsets)\n",
    "\n",
    "        cont, cont_offsets = self._prepare_embedding_inputs(row['production_countries_idx'])\n",
    "        cont_e = self.prod_cont_embedding(cont, cont_offsets)\n",
    "\n",
    "        lang, lang_offsets = self._prepare_embedding_inputs(row['spoken_languages_idx'])\n",
    "        lang_e = self.lang_embedding(lang, lang_offsets)\n",
    "\n",
    "        kw, kw_offsets = self._prepare_embedding_inputs(row['keywords_idx'])\n",
    "        kw_e = self.word_embedding(kw, kw_offsets)\n",
    "\n",
    "        tag, tag_offsets = self._prepare_embedding_inputs(row['tagline_idx'])\n",
    "        tag_e = self.word_embedding(tag, tag_offsets)\n",
    "\n",
    "        ov, ov_offsets = self._prepare_embedding_inputs(row['overview_idx'])\n",
    "        ov_e = self.word_embedding(ov, ov_offsets)\n",
    "\n",
    "        # Scalar features as tensors (ensure shape is [batch_size, 1])\n",
    "        revenue = torch.tensor(row[\"revenue\"], dtype=torch.float32).unsqueeze(1)\n",
    "        budget = torch.tensor(row[\"budget\"], dtype=torch.float32).unsqueeze(1)\n",
    "        runtime = torch.tensor(row[\"runtime\"], dtype=torch.float32).unsqueeze(1)\n",
    "        adult_idx = torch.tensor(row[\"adult_idx\"], dtype=torch.bool).unsqueeze(1)\n",
    "        vote_average = torch.tensor(row[\"vote_average\"], dtype=torch.float32).unsqueeze(1)\n",
    "        vote_count = torch.tensor(row[\"vote_count\"], dtype=torch.float32).unsqueeze(1)\n",
    "        popularity = torch.tensor(row[\"popularity\"], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        # Concatenate all embeddings and scalar features\n",
    "        master_embedding = torch.cat([\n",
    "            genres_e,\n",
    "            comp_e,\n",
    "            cont_e,\n",
    "            lang_e,\n",
    "            kw_e,\n",
    "            tag_e,\n",
    "            ov_e,\n",
    "            revenue,\n",
    "            budget,\n",
    "            runtime,\n",
    "            adult_idx,\n",
    "            vote_average,\n",
    "            vote_count,\n",
    "            popularity\n",
    "        ], dim=1)\n",
    "\n",
    "        return self.fc(master_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df8d30f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e06f6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRXTransformer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, batch_first=True) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        # Placeholder for transformer logic\n",
    "        print(f\"Input shape to transformer: {x.shape}\")\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x)\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        print(f\"Output shape from transformer: {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c6532702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRXModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 n_heads: int,\n",
    "                 num_layers: int,\n",
    "                 hidden_size: int,\n",
    "                 num_list_features: int,\n",
    "                 num_scalar_features: int,\n",
    "                 n_genres: int, \n",
    "                 n_production_companies: int,\n",
    "                 n_production_countries: int,\n",
    "                 n_spoken_languages: int,\n",
    "                 movie_vocab_stoi: Dict[str, int],\n",
    "                 user_vocab_stoi: Dict[str, int],\n",
    "                 n_movies: int,\n",
    "                 n_words: int,\n",
    "                 seq_len: int = 4,\n",
    "                 dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.movie_embeddings = MovieEmbeddings(d_model, hidden_size, num_list_features, num_scalar_features, n_genres, n_production_companies, n_production_countries, n_spoken_languages, n_words)\n",
    "        self.movie_vocab_stoi = movie_vocab_stoi\n",
    "        self.user_vocab_stoi = user_vocab_stoi\n",
    "        self.transformer_encoder = TRXTransformer(hidden_size, n_heads=n_heads, num_layers=num_layers)\n",
    "        self.fc1 = nn.Linear(hidden_size*seq_len, n_movies)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # # rows['sequence_movie_ids'] -> List[List[Any]] of shape [batch_size, 4]\n",
    "        # batch_movie_ids = rows['sequence_movie_ids'].to_list()\n",
    "\n",
    "        # # Map movie IDs to vocab indices using movie_vocab_stoi\n",
    "        # indexed_ids = [\n",
    "        #     [movie_vocab_stoi.get(str(mid), movie_vocab_stoi['<unk>']) for mid in seq]\n",
    "        #     for seq in batch_movie_ids\n",
    "        # ]\n",
    "        \n",
    "        # Fetch the rows filtered by the movie IDs\n",
    "        movie_rows = [movies_df.filter(pl.col('movieId_idx').is_in(x[i])) for i in range(len(x))] # List[pl.DataFrame]\n",
    "        \n",
    "        # Get embeddings: [batch_size, seq_len, embedding_dim]\n",
    "        embeddings = torch.stack([self.movie_embeddings(row) for row in movie_rows], dim=0)\n",
    "\n",
    "        \n",
    "        # Pass Embeddings through the Transformer Encoder\n",
    "        transformer_output = self.transformer_encoder(embeddings)\n",
    "        # transformer_output shape: [batch_size, seq_len, d_model]\n",
    "\n",
    "        # Reshape transformer_output to [batch_size, seq_len * d_model]\n",
    "        transformer_output = transformer_output.view(transformer_output.size(0), -1)\n",
    "        # transformer_output shape: [batch_size, seq_len * d_model]\n",
    "        print(f\"Transformer output shape: {transformer_output.shape}\")\n",
    "\n",
    "        # Pass to fc1\n",
    "        fc1_output = self.fc1(transformer_output)\n",
    "        # fc1_output shape: [batch_size, n_movies]\n",
    "\n",
    "        # Apply dropout\n",
    "        fc1_output = self.dropout(fc1_output)\n",
    "        # fc1_output shape: [batch_size, n_movies]\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = F.softmax(fc1_output, dim=1)\n",
    "        # probabilities shape: [batch_size, n_movies]\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0d0235f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4]) torch.Size([16]) torch.Size([16, 4]) torch.Size([16])\n",
      "tensor([12029, 34224, 26889, 37526])\n",
      "Input shape to transformer: torch.Size([16, 4, 256])\n",
      "Output shape from transformer: torch.Size([16, 4, 256])\n",
      "Transformer output shape: torch.Size([16, 1024])\n",
      "torch.Size([16, 86494])\n"
     ]
    }
   ],
   "source": [
    "model = TRXModel(\n",
    "    d_model=16,\n",
    "    n_heads=4,\n",
    "    num_layers=4,\n",
    "    hidden_size=256,\n",
    "    num_list_features=7,\n",
    "    num_scalar_features=7,\n",
    "    n_genres=len_genres, \n",
    "    n_production_companies=len_prod_comp, \n",
    "    n_production_countries=len_prod_cont, \n",
    "    n_spoken_languages=len_langs, \n",
    "    movie_vocab_stoi=movie_vocab_stoi,\n",
    "    user_vocab_stoi=user_vocab_stoi,\n",
    "    n_movies=len(movie_vocab_stoi),\n",
    "    n_words=len_words\n",
    ")\n",
    "\n",
    "for idx, (movie_data, user_data, ratings_data, y_train) in enumerate(train_iter):\n",
    "    print(movie_data.shape, user_data.shape, ratings_data.shape, y_train.shape)\n",
    "    print(movie_data[0])\n",
    "    y = model(movie_data)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3c8a2add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 112.02M\n"
     ]
    }
   ],
   "source": [
    "# Total parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters in the model: {total_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRXModel(\n",
       "  (movie_embeddings): MovieEmbeddings(\n",
       "    (genres_embedding): EmbeddingBag(21, 32, mode='mean')\n",
       "    (prod_comp_embedding): EmbeddingBag(45546, 16, mode='mean')\n",
       "    (prod_cont_embedding): EmbeddingBag(201, 16, mode='mean')\n",
       "    (lang_embedding): EmbeddingBag(164, 16, mode='mean')\n",
       "    (word_embedding): EmbeddingBag(270246, 64, mode='mean')\n",
       "    (fc): Linear(in_features=279, out_features=256, bias=True)\n",
       "  )\n",
       "  (transformer_encoder): TRXTransformer(\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=1024, out_features=86494, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TRXModel(\n",
    "    d_model=16,\n",
    "    n_heads=4,\n",
    "    num_layers=4,\n",
    "    hidden_size=256,\n",
    "    num_list_features=7,\n",
    "    num_scalar_features=7,\n",
    "    n_genres=len_genres, \n",
    "    n_production_companies=len_prod_comp, \n",
    "    n_production_countries=len_prod_cont, \n",
    "    n_spoken_languages=len_langs, \n",
    "    movie_vocab_stoi=movie_vocab_stoi,\n",
    "    user_vocab_stoi=user_vocab_stoi,\n",
    "    n_movies=len(movie_vocab_stoi),\n",
    "    n_words=len_words\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, train_iter, epoch) -> None:\n",
    "    # Switch to training mode\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, (movie_data, _, _, target) in enumerate(train_iter):\n",
    "        # Load movie sequence and user id\n",
    "        movie_data = movie_data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Predict movies\n",
    "        output = model(movie_data) # [batch-size, n_movies]\n",
    "        \n",
    "        # Backpropogation process\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        # Results\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e8305490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_data: torch.Tensor) -> float:\n",
    "    # Switch the model to evaluation mode.\n",
    "    # This is necessary for layers like dropout,\n",
    "    model.eval() \n",
    "    total_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (movie_data, _, _, target) in enumerate(eval_data):\n",
    "            # Load movie sequence and user id\n",
    "            movie_data = movie_data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            # Predict movies\n",
    "            output = model(movie_data)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape to transformer: torch.Size([16, 4, 256])\n",
      "Output shape from transformer: torch.Size([16, 4, 256])\n",
      "Transformer output shape: torch.Size([16, 1024])\n",
      "Input shape to transformer: torch.Size([16, 4, 256])\n",
      "Output shape from transformer: torch.Size([16, 4, 256])\n",
      "Transformer output shape: torch.Size([16, 1024])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[0;32m     11\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_iter)\n",
      "Cell \u001b[1;32mIn[80], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_iter, epoch)\u001b[0m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Tuffy\\Documents\\RecSysProject\\RecSysTRX\\.venv\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tuffy\\Documents\\RecSysProject\\RecSysTRX\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tuffy\\Documents\\RecSysProject\\RecSysTRX\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Training\n",
    "        train(model, train_iter, epoch)\n",
    "\n",
    "        # Evaluation\n",
    "        val_loss = evaluate(model, val_iter)\n",
    "\n",
    "        # Compute the perplexity of the validation loss\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "\n",
    "        # Results\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
