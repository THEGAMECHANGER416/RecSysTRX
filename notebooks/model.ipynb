{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6f6f4a",
   "metadata": {},
   "source": [
    "# Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e39adef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import math\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8ddfb",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58200b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pl.read_parquet('../data/processed/output.parquet')\n",
    "sequence_df = pl.read_parquet('../data/processed/train.parquet')\n",
    "vocabs = torch.load('../data/processed/all_vocabs.pth')\n",
    "\n",
    "user_vocab = vocabs[\"user_vocab\"]\n",
    "movie_vocab = vocabs[\"movie_vocab\"]\n",
    "genres_vocab = vocabs[\"genres_vocab\"]\n",
    "prod_comp_vocab = vocabs[\"prod_comp_vocab\"]\n",
    "prod_countries_vocab = vocabs[\"prod_countries_vocab\"]\n",
    "languages_vocab = vocabs[\"languages_vocab\"]\n",
    "words_vocab = vocabs[\"words_vocab\"]\n",
    "\n",
    "vocabs = {\n",
    "    \"user_vocab\": user_vocab,\n",
    "    \"movie_vocab\": movie_vocab,\n",
    "    \"genres_vocab\": genres_vocab,\n",
    "    \"prod_comp_vocab\": prod_comp_vocab,\n",
    "    \"prod_countries_vocab\": prod_countries_vocab,\n",
    "    \"languages_vocab\": languages_vocab,\n",
    "    \"words_vocab\": words_vocab,\n",
    "}\n",
    "\n",
    "movie_vocab_stoi = movie_vocab.get_stoi()\n",
    "user_vocab_stoi = user_vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9460c261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies DF shape: (86493, 15)\n",
      "Sequence DF shape: (13218442, 3)\n",
      "Vocab sizes: {'user_vocab': 200949, 'movie_vocab': 86494, 'genres_vocab': 21, 'prod_comp_vocab': 45546, 'prod_countries_vocab': 201, 'languages_vocab': 164, 'words_vocab': 270246}\n",
      "\n",
      "First movie row: {'movieId_idx': [29614], 'genres_idx': [[9, 12, 1]], 'production_companies_idx': [[5787, 10790, 33238]], 'production_countries_idx': [[199, 179]], 'spoken_languages_idx': [[6, 23, 137, 47]], 'keywords_idx': [[16308, 175809, 93150, 146205, 199438, 206752, 33269, 147659, 198442, 112495, 215262, 104288, 10122, 84395, 101318, 37337, 35868, 263713, 52720, 82866]], 'overview_idx': [[176485, 24219, 129031, 130595, 204009, 102295, 187482, 46191, 265111, 75891, 120691, 82866, 7607, 80603, 210357, 60107, 248412, 24219, 171073, 185742, 207323, 80603, 208294, 186120, 230514, 250658, 48946, 24219, 162914, 55906, 185742, 145939, 86280, 230898, 120691, 50712, 7607, 233356, 69050, 257026, 247940, 24219, 99177, 12348]], 'tagline_idx': [[214081, 63727, 60107, 120691, 122910, 7607, 120691, 55301]], 'adult_idx': [0], 'vote_average': [8.364], 'vote_count': [34495], 'revenue': [8255.327640000001], 'runtime': [148], 'budget': [1600.0000000000002], 'popularity': [83.952]}\n",
      "First sequence row: {'userId': ['user_1'], 'sequence_movie_ids': [['movie_2966', 'movie_2890', 'movie_3078', 'movie_2882', 'movie_541']], 'sequence_ratings': [[1.0, 4.0, 2.0, 1.0, 5.0]]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Movies DF shape: {movies_df.shape}\")\n",
    "print(f\"Sequence DF shape: {sequence_df.shape}\")\n",
    "print(\"Vocab sizes:\", {k: len(v) for k, v in vocabs.items()})\n",
    "print(\"\\nFirst movie row:\", movies_df[0].to_dict(as_series=False))\n",
    "print(\"First sequence row:\", sequence_df[0].to_dict(as_series=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55ad7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get Vocab Sizes for Model ---\n",
    "len_genres = len(genres_vocab)\n",
    "len_prod_comp = len(prod_comp_vocab)\n",
    "len_prod_cont = len(prod_countries_vocab)\n",
    "len_langs = len(languages_vocab)\n",
    "len_words = len(words_vocab)\n",
    "n_items = len(movie_vocab) # Total number of items including padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "839e37ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Padding Values ---\n",
    "PADDING_MOVIE_ID_INT = movie_vocab_stoi['<unk>'] # Integer ID for padding (should be 0)\n",
    "RATING_PADDING_VALUE = -1.0 # Rating used for padded positions\n",
    "\n",
    "assert PADDING_MOVIE_ID_INT == 0, \"Padding token for movie vocab should be 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a728e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created movie_features_map with 86494 entries (including padding ID 0).\n"
     ]
    }
   ],
   "source": [
    "# Map movie_id_int (from movies_df) to its features dictionary\n",
    "movie_features_map = {}\n",
    "\n",
    "# Define the list and scalar feature columns expected by MovieEmbeddings\n",
    "list_feature_cols = ['genres_idx', 'production_companies_idx', 'production_countries_idx',\n",
    "                     'spoken_languages_idx', 'keywords_idx', 'tagline_idx', 'overview_idx']\n",
    "scalar_feature_cols = ['revenue', 'budget', 'runtime', 'adult_idx',\n",
    "                       'vote_average', 'vote_count', 'popularity']\n",
    "\n",
    "# Iterate through movies_df to build the map using movieId_idx as key\n",
    "for row in movies_df.iter_rows(named=True):\n",
    "    movie_id_int = row['movieId_idx']\n",
    "    features = {}\n",
    "    for col in list_feature_cols + scalar_feature_cols:\n",
    "        features[col] = row[col]\n",
    "    movie_features_map[movie_id_int] = features\n",
    "\n",
    "# Add a special entry for the padding ID (0)\n",
    "# Features for padding should represent a neutral state\n",
    "padding_features = {}\n",
    "for col in list_feature_cols:\n",
    "    padding_features[col] = [] # Empty list\n",
    "for col in scalar_feature_cols:\n",
    "     # Use appropriate default/padding values for scalars (e.g., 0.0)\n",
    "     padding_features[col] = 0.0\n",
    "if 'adult_idx' in scalar_feature_cols: # Handle boolean padding\n",
    "    padding_features['adult_idx'] = False\n",
    "\n",
    "movie_features_map[PADDING_MOVIE_ID_INT] = padding_features\n",
    "\n",
    "print(f\"Created movie_features_map with {len(movie_features_map)} entries (including padding ID {PADDING_MOVIE_ID_INT}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58961c6",
   "metadata": {},
   "source": [
    "# Making Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "026c1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, sequence_df: pl.DataFrame, movie_vocab_stoi: Dict[str, int],\n",
    "                 padding_id: int = 0, rating_padding_value: float = -1.0):\n",
    "        self.sequence_df = sequence_df\n",
    "        self.movie_vocab_stoi = movie_vocab_stoi\n",
    "        self.padding_id = padding_id\n",
    "        self.rating_padding_value = rating_padding_value\n",
    "        self.sequence_length = 5 # Fixed sequence length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, List[Any]]:\n",
    "        row = self.sequence_df[idx].to_dict(as_series=False) # Get row as dict\n",
    "\n",
    "        # Extract string IDs and ratings lists\n",
    "        movie_ids_str = row['sequence_movie_ids'][0] # Assuming they are lists of lists from Polars\n",
    "        ratings = row['sequence_ratings'][0]\n",
    "\n",
    "        # Convert string movie IDs to integer IDs\n",
    "        movie_ids_int = [self.movie_vocab_stoi.get(uid, self.padding_id) for uid in movie_ids_str] # Use .get with default padding_id\n",
    "\n",
    "        # Ensure length is exactly 5, pad if necessary (should be handled in data prep, but safety check)\n",
    "        current_len = len(movie_ids_int)\n",
    "        if current_len < self.sequence_length:\n",
    "             movie_ids_int.extend([self.padding_id] * (self.sequence_length - current_len))\n",
    "             ratings.extend([self.rating_padding_value] * (self.sequence_length - current_len))\n",
    "        elif current_len > self.sequence_length:\n",
    "             movie_ids_int = movie_ids_int[:self.sequence_length]\n",
    "             ratings = ratings[:self.sequence_length]\n",
    "\n",
    "        # Return the 5 movie IDs and 5 ratings\n",
    "        return {'movie_ids': movie_ids_int, 'ratings': ratings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e6a7b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieCollator:\n",
    "    def __init__(self, movie_features_map: Dict[int, Dict[str, Any]],\n",
    "                 list_feature_cols: List[str], scalar_feature_cols: List[str],\n",
    "                 padding_id: int = 0, rating_padding_value: float = -1.0):\n",
    "        self.movie_features_map = movie_features_map\n",
    "        self.list_feature_cols = list_feature_cols\n",
    "        self.scalar_feature_cols = scalar_feature_cols\n",
    "        self.padding_id = padding_id\n",
    "        self.rating_padding_value = rating_padding_value\n",
    "        self.input_sequence_length = 4 # Input length is fixed at 4\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Custom collate function. Takes a batch of 5-item sequence samples and\n",
    "        prepares a batch for TRXModel (4 input items, 1 target).\n",
    "        \"\"\"\n",
    "        batch_size = len(batch)\n",
    "        input_seq_len = self.input_sequence_length # 4\n",
    "        total_input_items_in_batch = batch_size * input_seq_len\n",
    "\n",
    "        # --- 4.1 Extract and separate input/target from batch ---\n",
    "        batch_input_movie_ids: List[List[int]] = [] # List of 4-item lists\n",
    "        batch_input_ratings: List[List[float]] = [] # List of 4-item lists\n",
    "        batch_target_item_ids: List[int] = []       # List of 1 target ID\n",
    "\n",
    "        for sample in batch:\n",
    "            # Sample has 5 movie_ids and 5 ratings\n",
    "            batch_input_movie_ids.append(sample['movie_ids'][:input_seq_len]) # First 4 for input\n",
    "            batch_input_ratings.append(sample['ratings'][:input_seq_len])     # First 4 ratings\n",
    "            batch_target_item_ids.append(sample['movie_ids'][input_seq_len])  # The 5th item is the target\n",
    "\n",
    "        # --- 4.2 Determine padding mask for input sequences (length 4) ---\n",
    "        # Padding mask is True where the item ID is the padding_id (0)\n",
    "        padding_mask = torch.tensor(\n",
    "            [[id == self.padding_id for id in seq] for seq in batch_input_movie_ids],\n",
    "            dtype=torch.bool\n",
    "        ) # Shape: [Batch_Size, 4]\n",
    "\n",
    "\n",
    "        # --- 4.3 Collect features for all input items (B*4 total) into a flat pool ---\n",
    "        # This pool will contain features for all non-padded items.\n",
    "        # Padded positions in the sequence will map to a special index outside this pool.\n",
    "\n",
    "        pooled_list_features_raw: Dict[str, List[List[Any]]] = {col: [] for col in self.list_feature_cols}\n",
    "        pooled_scalar_features_raw: Dict[str, List[Any]] = {col: [] for col in self.scalar_feature_cols}\n",
    "        sequence_item_pool_map: List[List[int]] = [] # Map (batch_idx, seq_idx) -> pool_idx\n",
    "\n",
    "        current_pool_index = 0\n",
    "        # The index for padded items will be the total number of *non-padded* items in the pool.\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            current_sequence_map = []\n",
    "            for s in range(input_seq_len):\n",
    "                movie_id_int = batch_input_movie_ids[b][s]\n",
    "                is_padded = padding_mask[b][s].item()\n",
    "\n",
    "                if is_padded:\n",
    "                    # Map padded positions to the index *after* the real pool\n",
    "                    # We'll use actual_items_in_pool as the padding map index later\n",
    "                    # Add a placeholder index for now, fix it after calculating actual_items_in_pool\n",
    "                     current_sequence_map.append(-1) # Use a temporary invalid index\n",
    "                else:\n",
    "                    # Get features for the actual movie ID\n",
    "                    features = self.movie_features_map.get(movie_id_int)\n",
    "                    if features is None:\n",
    "                        # Fallback for safety, though map should include all relevant IDs + padding\n",
    "                        print(f\"Warning: Movie ID {movie_id_int} not found in movie_features_map. Using padding features.\")\n",
    "                        features = self.movie_features_map[self.padding_id]\n",
    "\n",
    "                    # Add features to the raw pooled lists/values\n",
    "                    for col in self.list_feature_cols:\n",
    "                        pooled_list_features_raw[col].append(features[col])\n",
    "                    for col in self.scalar_feature_cols:\n",
    "                        pooled_scalar_features_raw[col].append(features[col])\n",
    "\n",
    "                    # Map this sequence position to its index in the pool\n",
    "                    current_sequence_map.append(current_pool_index)\n",
    "                    current_pool_index += 1 # Increment pool index for the next non-padded item\n",
    "\n",
    "            sequence_item_pool_map.append(current_sequence_map)\n",
    "\n",
    "        # The actual number of non-padded items added to the pool\n",
    "        actual_items_in_pool = current_pool_index\n",
    "\n",
    "        # Now, fix the padding indices in sequence_item_pool_map to point to actual_items_in_pool\n",
    "        # This index will map to the dummy zero embedding later\n",
    "        sequence_item_pool_map_fixed = []\n",
    "        for seq_map in sequence_item_pool_map:\n",
    "             fixed_seq_map = [idx if idx != -1 else actual_items_in_pool for idx in seq_map]\n",
    "             sequence_item_pool_map_fixed.append(fixed_seq_map)\n",
    "\n",
    "        sequence_item_pool_map_tensor = torch.tensor(sequence_item_pool_map_fixed, dtype=torch.long) # Shape [Batch_Size, 4]\n",
    "\n",
    "\n",
    "        # --- 4.4 Prepare pooled features tensors for MovieEmbeddings ---\n",
    "        input_item_pool_data: Dict[str, Tuple[torch.Tensor, torch.Tensor] | torch.Tensor] = {}\n",
    "\n",
    "        # Process list features\n",
    "        for col in self.list_feature_cols:\n",
    "            flat_list = []\n",
    "            offsets = [0]\n",
    "            # Only process collected lists for non-padded items (size actual_items_in_pool)\n",
    "            for sublist in pooled_list_features_raw[col]:\n",
    "                flat_list.extend(sublist)\n",
    "                offsets.append(offsets[-1] + len(sublist))\n",
    "            offsets = offsets[:-1] # Remove the last cumulative sum\n",
    "\n",
    "            # Convert to tensors\n",
    "            # If actual_items_in_pool is 0, flat_list and offsets will be empty\n",
    "            if not flat_list:\n",
    "                 flat_tensor = torch.empty(0, dtype=torch.long)\n",
    "            else:\n",
    "                 flat_tensor = torch.tensor(flat_list, dtype=torch.long)\n",
    "\n",
    "            if actual_items_in_pool > 0 and flat_tensor.numel() == 0:\n",
    "                 # Case where there are non-padded items, but all their lists were empty\n",
    "                 offsets_tensor = torch.zeros(actual_items_in_pool, dtype=torch.long)\n",
    "            elif actual_items_in_pool == 0:\n",
    "                 # Case where all input items in the batch were padded\n",
    "                 offsets_tensor = torch.empty(0, dtype=torch.long)\n",
    "            else:\n",
    "                 offsets_tensor = torch.tensor(offsets, dtype=torch.long)\n",
    "\n",
    "            input_item_pool_data[col] = (flat_tensor, offsets_tensor)\n",
    "\n",
    "\n",
    "        # Process scalar features\n",
    "        for col in self.scalar_feature_cols:\n",
    "            values = pooled_scalar_features_raw[col] # Contains values for actual_items_in_pool\n",
    "            if len(values) > 0:\n",
    "                 if isinstance(values[0], bool):\n",
    "                      scalar_tensor = torch.tensor(values, dtype=torch.float32).unsqueeze(1)\n",
    "                 else:\n",
    "                      scalar_tensor = torch.tensor(values, dtype=torch.float32).unsqueeze(1)\n",
    "            else:\n",
    "                 scalar_tensor = torch.empty(0, 1, dtype=torch.float32)\n",
    "\n",
    "            input_item_pool_data[col] = scalar_tensor\n",
    "\n",
    "\n",
    "        # --- 4.5 Prepare input ratings and target item IDs tensors ---\n",
    "        input_ratings_tensor = torch.tensor(batch_input_ratings, dtype=torch.float32) # Shape [Batch_Size, 4]\n",
    "        target_item_ids_tensor = torch.tensor(batch_target_item_ids, dtype=torch.long) # Shape [Batch_Size]\n",
    "\n",
    "\n",
    "        # --- 4.6 Return the collated batch dictionary ---\n",
    "        return {\n",
    "            'input_item_pool_data': input_item_pool_data,\n",
    "            'input_sequence_info': {\n",
    "                'padding_mask': padding_mask,\n",
    "                'sequence_item_pool_map': sequence_item_pool_map_tensor,\n",
    "                'actual_items_in_pool': actual_items_in_pool # Pass actual size for dummy vector indexing\n",
    "            },\n",
    "            'input_ratings': input_ratings_tensor,\n",
    "            'target_item_ids': target_item_ids_tensor\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b1c31",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1603ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieEmbeddings(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model_internal: int, # Internal embedding dim before FC\n",
    "                 hidden_size: int,      # Output dim after FC (Item_Feature_Dim)\n",
    "                 n_genres: int,\n",
    "                 n_production_companies: int,\n",
    "                 n_production_countries: int,\n",
    "                 n_spoken_languages: int,\n",
    "                 n_words: int):\n",
    "        super().__init__()\n",
    "        # EmbeddingBag layers using provided vocab sizes\n",
    "        self.genres_embedding = nn.EmbeddingBag(n_genres, d_model_internal*2, mode='mean')\n",
    "        self.prod_comp_embedding = nn.EmbeddingBag(n_production_companies, d_model_internal, mode='mean')\n",
    "        self.prod_cont_embedding = nn.EmbeddingBag(n_production_countries, d_model_internal, mode='mean')\n",
    "        self.lang_embedding = nn.EmbeddingBag(n_spoken_languages, d_model_internal, mode='mean')\n",
    "        self.word_embedding = nn.EmbeddingBag(n_words, d_model_internal*4, mode='mean') # Shared for keywords, tagline, overview\n",
    "\n",
    "        # Calculate the input dimension for the final linear layer\n",
    "        total_embedding_dim = (d_model_internal * 2) + (d_model_internal * 1) + (d_model_internal * 1) + (d_model_internal * 1) + \\\n",
    "                              (d_model_internal * 4) + (d_model_internal * 4) + (d_model_internal * 4) # genres, prod_comp, prod_cont, lang, keywords, tagline, overview\n",
    "        num_scalar_features_actual = 7 # revenue, budget, runtime, adult_idx, vote_average, vote_count, popularity\n",
    "        fc_input_dim = total_embedding_dim + num_scalar_features_actual\n",
    "\n",
    "        self.fc = nn.Linear(fc_input_dim, hidden_size) # Output is Item_Feature_Dim\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, item_data: Dict[str, Tuple[torch.Tensor, torch.Tensor] | torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Processes pre-collated movie features for a pool of items.\n",
    "        Returns embeddings of shape [Total_Items_in_Pool, hidden_size].\n",
    "        \"\"\"\n",
    "        # The collate_fn ensures item_data contains tensors for the actual non-padded items\n",
    "        # + possibly empty tensors/offsets if actual_items_in_pool is 0\n",
    "\n",
    "        # --- Process List Features ---\n",
    "        genres_flat, genres_offsets = item_data['genres_idx']\n",
    "        # Use the number of offsets as batch size for EmbeddingBag if flat is empty\n",
    "        embag_batch_size = genres_offsets.shape[0] if genres_flat.numel() == 0 else None\n",
    "\n",
    "        genres_e = self.genres_embedding(genres_flat, genres_offsets) if genres_flat.numel() > 0 else torch.zeros(embag_batch_size if embag_batch_size is not None else 0, self.genres_embedding.embedding_dim, device=genres_flat.device)\n",
    "\n",
    "        comp_flat, comp_offsets = item_data['production_companies_idx']\n",
    "        embag_batch_size = comp_offsets.shape[0] if comp_flat.numel() == 0 else None\n",
    "        comp_e = self.prod_comp_embedding(comp_flat, comp_offsets) if comp_flat.numel() > 0 else torch.zeros(embag_batch_size if embag_batch_size is not None else 0, self.prod_comp_embedding.embedding_dim, device=comp_flat.device)\n",
    "\n",
    "        cont_flat, cont_offsets = item_data['production_countries_idx']\n",
    "        embag_batch_size = cont_offsets.shape[0] if cont_flat.numel() == 0 else None\n",
    "        cont_e = self.prod_cont_embedding(cont_flat, cont_offsets) if cont_flat.numel() > 0 else torch.zeros(embag_batch_size if embag_batch_size is not None else 0, self.prod_cont_embedding.embedding_dim, device=cont_flat.device)\n",
    "\n",
    "        lang_flat, lang_offsets = item_data['spoken_languages_idx']\n",
    "        embag_batch_size = lang_offsets.shape[0] if lang_flat.numel() == 0 else None\n",
    "        lang_e = self.lang_embedding(lang_flat, lang_offsets) if lang_flat.numel() > 0 else torch.zeros(embag_batch_size if embag_batch_size is not None else 0, self.lang_embedding.embedding_dim, device=lang_flat.device)\n",
    "\n",
    "        kw_flat, kw_offsets = item_data['keywords_idx']\n",
    "        embag_batch_size = kw_offsets.shape[0] if kw_flat.numel() == 0 else None\n",
    "        kw_e = self.word_embedding(kw_flat, kw_offsets) if kw_flat.numel() > 0 else torch.zeros(embag_batch_size if embag_batch_size is not None else 0, self.word_embedding.embedding_dim, device=kw_flat.device)\n",
    "\n",
    "        tag_flat, tag_offsets = item_data['tagline_idx']\n",
    "        embag_batch_size = tag_offsets.shape[0] if tag_flat.numel() == 0 else None\n",
    "        tag_e = self.word_embedding(tag_flat, tag_offsets) if tag_flat.numel() > 0 else torch.zeros(embag_batch_size if embag_batch_size is not None else 0, self.word_embedding.embedding_dim, device=tag_flat.device)\n",
    "\n",
    "        ov_flat, ov_offsets = item_data['overview_idx']\n",
    "        embag_batch_size = ov_offsets.shape[0] if ov_flat.numel() == 0 else None\n",
    "        ov_e = self.word_embedding(ov_flat, ov_offsets) if ov_flat.numel() > 0 else torch.zeros(embag_batch_size if embag_batch_size is not None else 0, self.word_embedding.embedding_dim, device=ov_flat.device)\n",
    "\n",
    "        # --- Process Scalar Features ---\n",
    "        revenue = item_data[\"revenue\"].float()\n",
    "        budget = item_data[\"budget\"].float()\n",
    "        runtime = item_data[\"runtime\"].float()\n",
    "        adult_idx = item_data[\"adult_idx\"].float()\n",
    "        vote_average = item_data[\"vote_average\"].float()\n",
    "        vote_count = item_data[\"vote_count\"].float()\n",
    "        popularity = item_data[\"popularity\"].float()\n",
    "\n",
    "        # --- Concatenate all embeddings and scalar features ---\n",
    "        # All these tensors should have actual_items_in_pool as their first dimension (or be empty if 0)\n",
    "        pooled_embedding = torch.cat([\n",
    "            genres_e, comp_e, cont_e, lang_e, kw_e, tag_e, ov_e,\n",
    "            revenue, budget, runtime, adult_idx, vote_average, vote_count, popularity\n",
    "        ], dim=1)\n",
    "\n",
    "        # Pass through final linear layer\n",
    "        return self.fc(pooled_embedding) # Shape: [Total_Items_in_Pool, hidden_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db000ba6",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ef762cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, sequence_length: int = 4):\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.pe = nn.Embedding(sequence_length, d_model)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "         nn.init.xavier_uniform_(self.pe.weight)\n",
    "\n",
    "    def forward(self, device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"Returns positional embeddings for the fixed input sequence length.\"\"\"\n",
    "        positions = torch.arange(self.sequence_length, dtype=torch.long, device=device)\n",
    "        return self.pe(positions) # Shape: [4, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670bcb6",
   "metadata": {},
   "source": [
    "# TRXModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88f03d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRXModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 movie_embedding_params: Dict[str, Any], # Params for MovieEmbeddings\n",
    "                 item_feature_dim: int,       # Output dimension of MovieEmbeddings (its hidden_size)\n",
    "                 transformer_n_heads: int,\n",
    "                 transformer_n_layers: int,\n",
    "                 transformer_dim_feedforward: int,\n",
    "                 transformer_dropout: float,\n",
    "                 n_items: int):               # Total number of unique items/movies for the output prediction layer\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_items = n_items\n",
    "        # Transformer dimension = Item Feature Dim + Rating Dim (1)\n",
    "        self.transformer_d_model = item_feature_dim + 1\n",
    "        # Ensure transformer_d_model is divisible by n_heads\n",
    "        assert self.transformer_d_model % transformer_n_heads == 0, \\\n",
    "            f\"Transformer d_model ({self.transformer_d_model}) must be divisible by n_heads ({transformer_n_heads})\"\n",
    "\n",
    "        self.input_sequence_length = 4 # Fixed input sequence length\n",
    "\n",
    "        # 1. Item Feature Embedding Layer (your detailed MovieEmbeddings)\n",
    "        # The output size of MovieEmbeddings's FC layer must be item_feature_dim\n",
    "        # Ensure 'hidden_size' in movie_embedding_params is set to item_feature_dim\n",
    "        self.movie_embeddings = MovieEmbeddings(**movie_embedding_params, hidden_size=item_feature_dim)\n",
    "\n",
    "        # 2. Positional Encoding (for fixed length 4)\n",
    "        self.positional_encoding = SimplePositionalEncoding(self.transformer_d_model, self.input_sequence_length)\n",
    "\n",
    "        # 3. Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.transformer_d_model,\n",
    "            nhead=transformer_n_heads,\n",
    "            dim_feedforward=transformer_dim_feedforward,\n",
    "            dropout=transformer_dropout,\n",
    "            batch_first=True # Set to True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=transformer_n_layers\n",
    "        )\n",
    "\n",
    "        # 4. Prediction Head: Predict scores for next item (the 5th item)\n",
    "        self.prediction_head = nn.Linear(self.transformer_d_model, n_items)\n",
    "\n",
    "        self.dropout = nn.Dropout(transformer_dropout)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.prediction_head.weight)\n",
    "        nn.init.zeros_(self.prediction_head.bias)\n",
    "        # MovieEmbeddings and PositionalEncoding have their own init\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                input_item_pool_data: Dict[str, Tuple[torch.Tensor, torch.Tensor] | torch.Tensor],\n",
    "                input_sequence_info: Dict[str, torch.Tensor],\n",
    "                input_ratings: torch.Tensor, # [Batch_Size, 4]\n",
    "                target_item_ids: torch.Tensor # [Batch_Size]\n",
    "               ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer Recommender Model.\n",
    "\n",
    "        Args:\n",
    "            input_item_pool_data: Dictionary containing pre-collated feature data for the\n",
    "                                  pool of input items.\n",
    "            input_sequence_info: Dictionary with 'padding_mask' [B, 4] and\n",
    "                                 'sequence_item_pool_map' [B, 4], 'actual_items_in_pool'.\n",
    "            input_ratings: Tensor [Batch_Size, 4] with ratings for the input items.\n",
    "            target_item_ids: Tensor [Batch_Size] with the IDs of the 5th item (target).\n",
    "\n",
    "        Returns:\n",
    "            Tuple: (item_scores [Batch_Size, n_items], target_item_ids [Batch_Size])\n",
    "        \"\"\"\n",
    "        device = input_ratings.device\n",
    "        batch_size = input_ratings.shape[0]\n",
    "        input_seq_len = self.input_sequence_length # which is 4\n",
    "\n",
    "        # 1. Get embeddings for the pool of input items\n",
    "        # Output shape: [Actual_Items_in_Pool, item_feature_dim]\n",
    "        pooled_item_features = self.movie_embeddings(input_item_pool_data)\n",
    "        actual_items_in_pool = input_sequence_info['actual_items_in_pool'] # Convert to int\n",
    "\n",
    "        # 2. Prepare pooled features with a dummy vector for padding\n",
    "        # The dummy vector index in sequence_item_pool_map is actual_items_in_pool\n",
    "        dummy_zero_feature = torch.zeros(1, self.movie_embeddings.fc.out_features, device=device)\n",
    "        pooled_item_features_with_dummy = torch.cat([pooled_item_features, dummy_zero_feature], dim=0) # Shape: [Actual_Items_in_Pool + 1, Item_Feature_Dim]\n",
    "\n",
    "\n",
    "        # 3. Reshape/Gather pooled features back into sequence structure\n",
    "        # Use the map to fill the sequence tensor [Batch_Size, 4, item_feature_dim]\n",
    "        sequence_item_pool_map = input_sequence_info['sequence_item_pool_map'].long() # [Batch_Size, 4]\n",
    "\n",
    "        # Gather features using the map indices\n",
    "        # Flatten the map and gather from the pooled features with dummy\n",
    "        flat_map = sequence_item_pool_map.view(-1) # [Batch_Size * 4]\n",
    "        sequence_item_features_flat = torch.index_select(pooled_item_features_with_dummy, dim=0, index=flat_map)\n",
    "        sequence_item_features = sequence_item_features_flat.view(batch_size, input_seq_len, -1) # Reshape back [B, 4, Item_Feature_Dim]\n",
    "\n",
    "        # Zero out features for padded positions if needed (dummy vector should handle this if it's truly zero)\n",
    "        # Using the padding mask from input_sequence_info\n",
    "        padding_mask = input_sequence_info['padding_mask'].unsqueeze(-1) # [B, 4, 1]\n",
    "        sequence_item_features = sequence_item_features * (~padding_mask)\n",
    "\n",
    "\n",
    "        # 4. Concatenate Features and Ratings\n",
    "        # sequence_item_features: [Batch_Size, 4, item_feature_dim]\n",
    "        # input_ratings: [Batch_Size, 4] -> needs to be [Batch_Size, 4, 1]\n",
    "        input_ratings_expanded = input_ratings.unsqueeze(-1) # Shape: [Batch_Size, 4, 1]\n",
    "\n",
    "        # Concatenate along the last dimension\n",
    "        sequence_input = torch.cat([sequence_item_features, input_ratings_expanded], dim=-1)\n",
    "        # Shape: [Batch_Size, 4, item_feature_dim + 1] = [Batch_Size, 4, transformer_d_model]\n",
    "\n",
    "\n",
    "        # 5. Add Positional Encoding (for fixed length 4)\n",
    "        # Positional embeddings shape: [4, transformer_d_model]\n",
    "        positional_embeddings = self.positional_encoding(device)\n",
    "        # Add positional embeddings to each item in the batch\n",
    "        # Shape: [Batch_Size, 4, transformer_d_model]\n",
    "        sequence_input = sequence_input + positional_embeddings.unsqueeze(0) # unsqueeze to broadcast over batch dim\n",
    "\n",
    "        # Apply dropout\n",
    "        sequence_input = self.dropout(sequence_input)\n",
    "\n",
    "        # 6. Pass through Transformer Encoder\n",
    "        # Input shape [Batch_Size, Sequence_Length, d_model] because batch_first=True\n",
    "        transformer_output = self.transformer_encoder(\n",
    "            sequence_input,\n",
    "            src_key_padding_mask=input_sequence_info['padding_mask'] # Pass the padding mask\n",
    "        ) # Output shape: [Batch_Size, 4, transformer_d_model]\n",
    "\n",
    "\n",
    "        # 7. Extract Representation for Prediction\n",
    "        # We predict the 5th item based on the representation of the 4th item (last input item)\n",
    "        # The index of the last input item is 3 (since input sequence length is 4, indices 0-3)\n",
    "        # We need the output embedding at index 3 for every sequence in the batch.\n",
    "        extracted_representation = transformer_output[:, 3, :] # Shape: [Batch_Size, transformer_d_model]\n",
    "\n",
    "        # Apply dropout before the final layer\n",
    "        extracted_representation = self.dropout(extracted_representation)\n",
    "\n",
    "        # 8. Prediction Head\n",
    "        # Predict scores for all items in the catalog\n",
    "        # Output shape: [Batch_Size, n_items]\n",
    "        item_scores = self.prediction_head(extracted_representation)\n",
    "\n",
    "        # Return scores and the target item IDs for loss calculation outside the forward pass\n",
    "        return item_scores, target_item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7096e11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model instantiated with 23737942 parameters.\n",
      "Transformer D_model: 64\n",
      "\n",
      "Iterating through DataLoader on device: cpu\n",
      "\n",
      "Processing batch 1\n",
      "Output scores shape: torch.Size([32, 86494])\n",
      "Returned target IDs shape: torch.Size([32])\n",
      "Dummy Loss: 11.367762565612793\n"
     ]
    }
   ],
   "source": [
    "# Model Parameters\n",
    "d_model_internal_me = 16 # Internal dim used in MovieEmbeddings\n",
    "# Choose item_feature_dim such that (item_feature_dim + 1) is divisible by transformer_n_heads\n",
    "item_feature_dim = 63 # Example, adjusted for n_heads=4 --> 63 + 1 = 64, divisible by 4\n",
    "transformer_n_heads = 4\n",
    "transformer_n_layers = 2\n",
    "ff_dimension = 128\n",
    "batch_size = 32 # Your desired batch size\n",
    "\n",
    "# MovieEmbeddings parameters using actual vocab sizes\n",
    "movie_embed_params = {\n",
    "    'd_model_internal': d_model_internal_me,\n",
    "    'n_genres': len_genres,\n",
    "    'n_production_companies': len_prod_comp,\n",
    "    'n_production_countries': len_prod_cont,\n",
    "    'n_spoken_languages': len_langs,\n",
    "    'n_words': len_words,\n",
    "    # hidden_size will be set by TRXModel based on item_feature_dim\n",
    "}\n",
    "\n",
    "\n",
    "# --- Instantiate Dataset and Collator ---\n",
    "movie_dataset = MovieDataset(sequence_df, movie_vocab_stoi,\n",
    "                                padding_id=PADDING_MOVIE_ID_INT,\n",
    "                                rating_padding_value=RATING_PADDING_VALUE)\n",
    "\n",
    "movie_collator = MovieCollator(movie_features_map, list_feature_cols, scalar_feature_cols,\n",
    "                                padding_id=PADDING_MOVIE_ID_INT,\n",
    "                                rating_padding_value=RATING_PADDING_VALUE)\n",
    "\n",
    "# --- Instantiate DataLoader ---\n",
    "# Use num_workers > 0 for faster data loading in practice\n",
    "movie_dataloader = DataLoader(\n",
    "    movie_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=movie_collator,\n",
    "    num_workers=0 # Set to >0 for performance, but 0 is easier for debugging\n",
    ")\n",
    "\n",
    "# --- Instantiate Model ---\n",
    "model = TRXModel(\n",
    "    movie_embedding_params=movie_embed_params,\n",
    "    item_feature_dim=item_feature_dim,\n",
    "    transformer_n_heads=transformer_n_heads,\n",
    "    transformer_n_layers=transformer_n_layers,\n",
    "    transformer_dim_feedforward=ff_dimension,\n",
    "    transformer_dropout=0.1,\n",
    "    n_items=n_items # Use total items from vocab\n",
    ")\n",
    "\n",
    "print(f\"Model instantiated with {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters.\")\n",
    "print(f\"Transformer D_model: {model.transformer_d_model}\")\n",
    "\n",
    "# --- Dummy Training Loop Iteration ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"\\nIterating through DataLoader on device: {device}\")\n",
    "\n",
    "try:\n",
    "    # Get one batch from the DataLoader\n",
    "    for batch_idx, batch_data in enumerate(movie_dataloader):\n",
    "        print(f\"\\nProcessing batch {batch_idx + 1}\")\n",
    "\n",
    "        # Move batch data to the device\n",
    "        # This involves iterating through the nested dictionary structure\n",
    "        batch_data_on_device = {}\n",
    "        for key, value in batch_data.items():\n",
    "            if key == 'input_item_pool_data':\n",
    "                batch_data_on_device[key] = {}\n",
    "                for feature_key, feature_value in value.items():\n",
    "                    if isinstance(feature_value, tuple): # (flat_indices, offsets)\n",
    "                        batch_data_on_device[key][feature_key] = (feature_value[0].to(device), feature_value[1].to(device))\n",
    "                    elif isinstance(feature_value, torch.Tensor): # Scalars\n",
    "                            batch_data_on_device[key][feature_key] = feature_value.to(device)\n",
    "                    else:\n",
    "                            batch_data_on_device[key][feature_key] = feature_value # Keep non-tensor items (like int actual_items_in_pool)\n",
    "            elif key == 'input_sequence_info':\n",
    "                    batch_data_on_device[key] = {}\n",
    "                    for info_key, info_value in value.items():\n",
    "                        if isinstance(info_value, torch.Tensor):\n",
    "                            batch_data_on_device[key][info_key] = info_value.to(device)\n",
    "                        else:\n",
    "                            batch_data_on_device[key][info_key] = info_value # Keep non-tensor (like int)\n",
    "            elif isinstance(value, torch.Tensor): # input_ratings, target_item_ids\n",
    "                batch_data_on_device[key] = value.to(device)\n",
    "            else:\n",
    "                batch_data_on_device[key] = value\n",
    "\n",
    "\n",
    "        # Separate input arguments for the model's forward method\n",
    "        input_item_pool_data = batch_data_on_device['input_item_pool_data']\n",
    "        input_sequence_info = batch_data_on_device['input_sequence_info']\n",
    "        input_ratings = batch_data_on_device['input_ratings']\n",
    "        target_item_ids = batch_data_on_device['target_item_ids']\n",
    "\n",
    "        # Perform a forward pass\n",
    "        # In a real training loop, remove torch.no_grad()\n",
    "        with torch.no_grad():\n",
    "            output_scores, returned_target_ids = model(\n",
    "            input_item_pool_data=input_item_pool_data,\n",
    "            input_sequence_info=input_sequence_info,\n",
    "            input_ratings=input_ratings,\n",
    "            target_item_ids=target_item_ids # Pass targets, model returns them\n",
    "            )\n",
    "\n",
    "        print(f\"Output scores shape: {output_scores.shape}\")\n",
    "        print(f\"Returned target IDs shape: {returned_target_ids.shape}\")\n",
    "\n",
    "        # Example Loss Calculation (in a real training loop)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(output_scores, returned_target_ids)\n",
    "        print(f\"Dummy Loss: {loss.item()}\")\n",
    "\n",
    "        # Stop after the first batch for this example\n",
    "        if batch_idx == 0:\n",
    "            break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during data loading or forward pass: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a367c4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
