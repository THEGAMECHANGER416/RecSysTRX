{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6f6f4a",
   "metadata": {},
   "source": [
    "# Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e39adef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from tempfile import TemporaryDirectory\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8ddfb",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58200b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arnav\\Documents\\VSCodeProjects\\RecSysTRX\\RecSysTRX\\.venv\\lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\arnav\\Documents\\VSCodeProjects\\RecSysTRX\\RecSysTRX\\.venv\\lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "movies_df = pl.read_parquet('../data/processed/output.parquet')\n",
    "train_df = pl.read_parquet('../data/processed/train.parquet')\n",
    "test_df = pl.read_parquet('../data/processed/test.parquet')\n",
    "vocabs = torch.load('../data/processed/all_vocabs.pth')\n",
    "\n",
    "user_vocab = vocabs[\"user_vocab\"]\n",
    "movie_vocab = vocabs[\"movie_vocab\"]\n",
    "genres_vocab = vocabs[\"genres_vocab\"]\n",
    "prod_comp_vocab = vocabs[\"prod_comp_vocab\"]\n",
    "prod_countries_vocab = vocabs[\"prod_countries_vocab\"]\n",
    "languages_vocab = vocabs[\"languages_vocab\"]\n",
    "words_vocab = vocabs[\"words_vocab\"]\n",
    "\n",
    "vocabs = {\n",
    "    \"user_vocab\": user_vocab,\n",
    "    \"movie_vocab\": movie_vocab,\n",
    "    \"genres_vocab\": genres_vocab,\n",
    "    \"prod_comp_vocab\": prod_comp_vocab,\n",
    "    \"prod_countries_vocab\": prod_countries_vocab,\n",
    "    \"languages_vocab\": languages_vocab,\n",
    "    \"words_vocab\": words_vocab,\n",
    "}\n",
    "\n",
    "movie_vocab_stoi = movie_vocab.get_stoi()\n",
    "user_vocab_stoi = user_vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9460c261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies DF shape: (86493, 15)\n",
      "Train DF shape: (100000, 3)\n",
      "Test DF shape: (1000, 3)\n",
      "Vocab sizes: {'user_vocab': 200949, 'movie_vocab': 86494, 'genres_vocab': 21, 'prod_comp_vocab': 45546, 'prod_countries_vocab': 201, 'languages_vocab': 164, 'words_vocab': 270246}\n",
      "\n",
      "First movie row: {'movieId_idx': [29614], 'genres_idx': [[9, 12, 1]], 'production_companies_idx': [[5787, 10790, 33238]], 'production_countries_idx': [[199, 179]], 'spoken_languages_idx': [[6, 23, 137, 47]], 'keywords_idx': [[16308, 175809, 93150, 146205, 199438, 206752, 33269, 147659, 198442, 112495, 215262, 104288, 10122, 84395, 101318, 37337, 35868, 263713, 52720, 82866]], 'overview_idx': [[176485, 24219, 129031, 130595, 204009, 102295, 187482, 46191, 265111, 75891, 120691, 82866, 7607, 80603, 210357, 60107, 248412, 24219, 171073, 185742, 207323, 80603, 208294, 186120, 230514, 250658, 48946, 24219, 162914, 55906, 185742, 145939, 86280, 230898, 120691, 50712, 7607, 233356, 69050, 257026, 247940, 24219, 99177, 12348]], 'tagline_idx': [[214081, 63727, 60107, 120691, 122910, 7607, 120691, 55301]], 'adult_idx': [0], 'vote_average': [8.364], 'vote_count': [34495], 'revenue': [8255.327640000001], 'runtime': [148], 'budget': [1600.0000000000002], 'popularity': [83.952]}\n",
      "First sequence row: {'userId': ['user_20007'], 'sequence_movie_ids': [['movie_64983', 'movie_106916', 'movie_4452', 'movie_41573', 'movie_108932', 'movie_36527', 'movie_79185', 'movie_70599', 'movie_46967', 'movie_43560', 'movie_102903', 'movie_68793']], 'sequence_ratings': [[2.5, 4.0, 3.0, 3.5, 3.0, 4.0, 2.5, 3.0, 4.0, 3.5, 3.0, 4.5]]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Movies DF shape: {movies_df.shape}\")\n",
    "print(f\"Train DF shape: {train_df.shape}\")\n",
    "print(f\"Test DF shape: {test_df.shape}\")\n",
    "print(\"Vocab sizes:\", {k: len(v) for k, v in vocabs.items()})\n",
    "print(\"\\nFirst movie row:\", movies_df[0].to_dict(as_series=False))\n",
    "print(\"First sequence row:\", train_df[0].to_dict(as_series=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55ad7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get Vocab Sizes for Model ---\n",
    "len_genres = len(genres_vocab)\n",
    "len_prod_comp = len(prod_comp_vocab)\n",
    "len_prod_cont = len(prod_countries_vocab)\n",
    "len_langs = len(languages_vocab)\n",
    "len_words = len(words_vocab)\n",
    "n_items = len(movie_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d32cc059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('userId', String),\n",
       "        ('sequence_movie_ids', List(String)),\n",
       "        ('sequence_ratings', List(Float64))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "180a2eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class MovieSeqDataset(Dataset):\n",
    "    def __init__(self, data, movie_vocab_stoi, user_vocab_stoi):\n",
    "        self.data = data\n",
    "        self.movie_vocab_stoi = movie_vocab_stoi\n",
    "        self.user_vocab_stoi = user_vocab_stoi\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user, movie_sequence, rating_sequence = self.data[idx]\n",
    "        movie_data = [self.movie_vocab_stoi.get(item,movie_vocab_stoi['<unk>']) for item in movie_sequence.to_list()[0]]\n",
    "        user_data = self.user_vocab_stoi[user.to_list()[0]]\n",
    "        rating_sequence = rating_sequence.to_list()[0]\n",
    "        return torch.tensor(movie_data[:-1],device='cuda'), torch.tensor(user_data), torch.tensor(rating_sequence[:-1]), torch.tensor(movie_data[-1],device='cuda')\n",
    "\n",
    "def collate_batch(batch):\n",
    "    movie_list = [item[0] for item in batch]\n",
    "    user_list = [item[1] for item in batch]\n",
    "    rating_list = [item[2] for item in batch]\n",
    "    target_list = [item[3] for item in batch]\n",
    "    return pad_sequence(movie_list, padding_value=movie_vocab_stoi['<unk>'], batch_first=True), torch.stack(user_list), pad_sequence(rating_list, padding_value=3, batch_first=True), torch.stack(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a64a911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "train_dataset = MovieSeqDataset(train_df, movie_vocab_stoi, user_vocab_stoi)\n",
    "val_dataset = MovieSeqDataset(test_df, movie_vocab_stoi, user_vocab_stoi)\n",
    "\n",
    "train_iter = DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True, collate_fn=collate_batch)\n",
    "val_iter = DataLoader(val_dataset, batch_size=BATCH_SIZE,shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f928e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 11]) torch.Size([256]) torch.Size([256, 11]) torch.Size([256])\n",
      "tensor([15837, 27579, 24675, 78108, 33476, 78820, 40442,  2674, 21279, 76577,\n",
      "        71546], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i, (movie_data, user_data, ratings_data, y_train) in enumerate(train_iter):\n",
    "    print(movie_data.shape, user_data.shape, ratings_data.shape, y_train.shape)\n",
    "    print(movie_data[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60dbc3a",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d3c03ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from typing import Tuple\n",
    "\n",
    "class MovieEmbeddings(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 hidden_size: int,\n",
    "                 num_list_features: int,\n",
    "                 num_scalar_features: int,\n",
    "                 n_genres: int, \n",
    "                 n_production_companies: int,\n",
    "                 n_production_countries: int,\n",
    "                 n_spoken_languages: int,\n",
    "                 n_words: int):\n",
    "        super().__init__()\n",
    "        self.genres_embedding = nn.EmbeddingBag(n_genres, d_model*2, mode='mean')\n",
    "        self.prod_comp_embedding = nn.EmbeddingBag(n_production_companies, d_model, mode='mean')\n",
    "        self.prod_cont_embedding = nn.EmbeddingBag(n_production_countries, d_model, mode='mean')\n",
    "        self.lang_embedding = nn.EmbeddingBag(n_spoken_languages, d_model, mode='mean')\n",
    "        self.word_embedding = nn.EmbeddingBag(n_words, d_model*4, mode='mean')\n",
    "        self.fc = nn.Linear(d_model*(10+num_list_features)+num_scalar_features,hidden_size)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.xavier_uniform_(self.genres_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.prod_comp_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.prod_cont_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.lang_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.word_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def _prepare_embedding_inputs(self, list_of_lists) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        flat_list = []\n",
    "        offsets = [0]\n",
    "        for sublist in list_of_lists:\n",
    "            flat_list.extend(sublist)\n",
    "            offsets.append(offsets[-1] + len(sublist))\n",
    "        offsets = offsets[:-1]  # Remove last cumulative sum\n",
    "        offsets = torch.tensor(offsets, dtype=torch.long,device='cuda')\n",
    "        flat_list = torch.tensor(flat_list, dtype=torch.long,device='cuda')\n",
    "        return flat_list, offsets   \n",
    "\n",
    "    def forward(self, row: pl.DataFrame) -> torch.Tensor:\n",
    "        genres, genres_offsets = self._prepare_embedding_inputs(row['genres_idx'])\n",
    "        genres_e = self.genres_embedding(genres, genres_offsets)\n",
    "\n",
    "        comp, comp_offsets = self._prepare_embedding_inputs(row['production_companies_idx'])\n",
    "        comp_e = self.prod_comp_embedding(comp, comp_offsets)\n",
    "\n",
    "        cont, cont_offsets = self._prepare_embedding_inputs(row['production_countries_idx'])\n",
    "        cont_e = self.prod_cont_embedding(cont, cont_offsets)\n",
    "\n",
    "        lang, lang_offsets = self._prepare_embedding_inputs(row['spoken_languages_idx'])\n",
    "        lang_e = self.lang_embedding(lang, lang_offsets)\n",
    "\n",
    "        kw, kw_offsets = self._prepare_embedding_inputs(row['keywords_idx'])\n",
    "        kw_e = self.word_embedding(kw, kw_offsets)\n",
    "\n",
    "        tag, tag_offsets = self._prepare_embedding_inputs(row['tagline_idx'])\n",
    "        tag_e = self.word_embedding(tag, tag_offsets)\n",
    "\n",
    "        ov, ov_offsets = self._prepare_embedding_inputs(row['overview_idx'])\n",
    "        ov_e = self.word_embedding(ov, ov_offsets)\n",
    "\n",
    "        # Scalar features as tensors (ensure shape is [batch_size, 1])\n",
    "        revenue = torch.tensor(row[\"revenue\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "        budget = torch.tensor(row[\"budget\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "        runtime = torch.tensor(row[\"runtime\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "        adult_idx = torch.tensor(row[\"adult_idx\"], dtype=torch.bool,device='cuda').unsqueeze(1)\n",
    "        vote_average = torch.tensor(row[\"vote_average\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "        vote_count = torch.tensor(row[\"vote_count\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "        popularity = torch.tensor(row[\"popularity\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "\n",
    "        # Concatenate all embeddings and scalar features\n",
    "        master_embedding = torch.cat([\n",
    "            genres_e,\n",
    "            comp_e,\n",
    "            cont_e,\n",
    "            lang_e,\n",
    "            kw_e,\n",
    "            tag_e,\n",
    "            ov_e,\n",
    "            revenue,\n",
    "            budget,\n",
    "            runtime,\n",
    "            adult_idx,\n",
    "            vote_average,\n",
    "            vote_count,\n",
    "            popularity\n",
    "        ], dim=1)\n",
    "\n",
    "        return self.fc(master_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df8d30f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e06f6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRXTransformer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, batch_first=True) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x)\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "c6532702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRXModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 n_heads: int,\n",
    "                 num_layers: int,\n",
    "                 hidden_size: int, # This is the output dimension of MovieEmbeddings' FC layer\n",
    "                 num_list_features: int,\n",
    "                 num_scalar_features: int,\n",
    "                 n_genres: int,\n",
    "                 n_production_companies: int,\n",
    "                 n_production_countries: int,\n",
    "                 n_spoken_languages: int,\n",
    "                 movie_vocab_stoi: Dict[str, int],\n",
    "                 user_vocab_stoi: Dict[str, int],\n",
    "                 n_movies: int,\n",
    "                 n_words: int,\n",
    "                 fc_size: int = 512,\n",
    "                 seq_len: int = 11,\n",
    "                 dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.num_list_features = num_list_features\n",
    "        self.num_scalar_features = num_scalar_features\n",
    "        self.n_genres = n_genres\n",
    "        self.n_production_companies = n_production_companies\n",
    "        self.n_production_countries = n_production_countries\n",
    "        self.n_spoken_languages = n_spoken_languages\n",
    "        self.n_movies = n_movies\n",
    "        self.n_words = n_words\n",
    "        self.fc_size = fc_size\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.movie_embeddings = MovieEmbeddings(d_model, hidden_size, num_list_features, num_scalar_features, n_genres, n_production_companies, n_production_countries, n_spoken_languages, n_words)\n",
    "        self.movie_vocab_stoi = movie_vocab_stoi\n",
    "        self.user_vocab_stoi = user_vocab_stoi\n",
    "\n",
    "        self.transformer_encoder = TRXTransformer(hidden_size, n_heads=n_heads, num_layers=num_layers)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size*seq_len, fc_size)\n",
    "        self.fc2 = nn.Linear(fc_size, n_movies)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self._init_weights()\n",
    "\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, movies_df: pl.DataFrame) -> torch.Tensor:\n",
    "        # x: [batch_size, seq_len]\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        sequence_movie_rows: List[pl.DataFrame] = [movies_df.filter(pl.col('movieId_idx').is_in(x[i].cpu().tolist())) for i in range(batch_size)]\n",
    "        embeddings = [self.movie_embeddings(row) for row in sequence_movie_rows] # List of tensors [seq_len, hidden_size]\n",
    "\n",
    "        # Stack and pad embeddings if sequences are different sizes\n",
    "        max_len = max([emb.shape[0] for emb in embeddings]) if embeddings else 0\n",
    "\n",
    "        # Handle empty batch or empty sequences gracefully\n",
    "        if max_len == 0:\n",
    "            return torch.zeros(batch_size, self.fc2.out_features, device=x.device)\n",
    "\n",
    "        padded_embeddings = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            current_seq_len = embeddings[i].shape[0]\n",
    "            if current_seq_len < max_len:\n",
    "                # Pad with zeros along the sequence dimension - NOW self.hidden_size is available\n",
    "                pad_tensor = torch.zeros(max_len - current_seq_len, self.hidden_size, device='cuda')\n",
    "                padded_embeddings.append(torch.cat([embeddings[i], pad_tensor], dim=0))\n",
    "            else:\n",
    "                padded_embeddings.append(embeddings[i][:max_len]) # Truncate if somehow longer\n",
    "\n",
    "        embeddings = torch.stack(padded_embeddings, dim=0)\n",
    "        # embeddings shape: [batch_size, max_len, hidden_size]\n",
    "\n",
    "        # Pass Embeddings through the Transformer Encoder\n",
    "        transformer_output = self.transformer_encoder(embeddings)\n",
    "        # transformer_output shape: [batch_size, max_len, hidden_size]\n",
    "\n",
    "        # Reshape transformer_output to feed into fc1\n",
    "        transformer_output = transformer_output.view(transformer_output.size(0), -1)\n",
    "        # transformer_output shape: [batch_size, max_len * hidden_size]\n",
    "\n",
    "        # Pass to fc1\n",
    "        fc1_output = self.fc1(transformer_output)\n",
    "        # fc1_output shape: [batch_size, fc_size]\n",
    "\n",
    "        # Apply dropout\n",
    "        fc1_output = self.dropout(fc1_output)\n",
    "\n",
    "        # Pass to fc2 (outputs scores for all n_movies)\n",
    "        fc2_output = self.fc2(fc1_output)\n",
    "        # fc2_output shape: [batch_size, n_movies]\n",
    "\n",
    "        return fc2_output # Return raw logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af6a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_negatives(positive_ids: torch.Tensor, n_movies: int, num_negatives: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Samples negative movie indices for each positive ID in the batch.\n",
    "    Ensures that negative samples do not include the corresponding positive ID.\n",
    "\n",
    "    Args:\n",
    "        positive_ids (torch.Tensor): Tensor of shape [batch_size] with positive movie indices.\n",
    "        n_movies (int): Total number of movies in the vocabulary.\n",
    "        num_negatives (int): Number of negative samples per positive.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape [batch_size, num_negatives] with negative movie indices.\n",
    "    \"\"\"\n",
    "    batch_size = positive_ids.size(0)\n",
    "    num_negatives = min(num_negatives, n_movies - 1)  # Ensure we don't ask for more than possible\n",
    "\n",
    "    # Create tensor of all possible movie IDs\n",
    "    all_movie_ids = torch.arange(n_movies, device=positive_ids.device)\n",
    "\n",
    "    # Allocate tensor for results\n",
    "    final_negative_ids = torch.empty(batch_size, num_negatives, dtype=torch.long, device=positive_ids.device)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        pos_id = positive_ids[i]  # No .item(), stays on GPU\n",
    "        # Exclude the positive ID from possible negatives\n",
    "        mask = all_movie_ids != pos_id\n",
    "        possible_negatives = all_movie_ids[mask]\n",
    "\n",
    "        if possible_negatives.numel() < num_negatives:\n",
    "            # Not enough negatives — warn and pad with zeros\n",
    "            print(f\"Warning: Not enough negatives for index {i}, only {possible_negatives.numel()} available.\")\n",
    "            sampled = torch.randperm(possible_negatives.numel(), device=positive_ids.device)\n",
    "            final_negative_ids[i, :possible_negatives.numel()] = possible_negatives[sampled]\n",
    "            final_negative_ids[i, possible_negatives.numel():] = 0  # Assumes 0 is a valid ID or placeholder\n",
    "        else:\n",
    "            # Sample without replacement from possible_negatives\n",
    "            sampled = torch.randperm(possible_negatives.numel(), device=positive_ids.device)[:num_negatives]\n",
    "            final_negative_ids[i] = possible_negatives[sampled]\n",
    "\n",
    "    return final_negative_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "33d692c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5574"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# movie_vocab_itos[85229]\n",
    "movie_vocab_stoi['movie_318']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "3b2e2389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>movieId_idx</th><th>genres_idx</th><th>production_companies_idx</th><th>production_countries_idx</th><th>spoken_languages_idx</th><th>keywords_idx</th><th>overview_idx</th><th>tagline_idx</th><th>adult_idx</th><th>vote_average</th><th>vote_count</th><th>revenue</th><th>runtime</th><th>budget</th><th>popularity</th></tr><tr><td>i64</td><td>list[i64]</td><td>list[i64]</td><td>list[i64]</td><td>list[i64]</td><td>list[i64]</td><td>list[i64]</td><td>list[i64]</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>5574</td><td>[11, 14]</td><td>[8939, 33821]</td><td>[64, 71]</td><td>[153, 23, … 6]</td><td>[199438, 206752, … 152019]</td><td>[2003, 96843, … 263100]</td><td>[1]</td><td>0</td><td>5.9</td><td>25</td><td>0.0</td><td>124</td><td>0.0</td><td>2.796</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 15)\n",
       "┌─────────────┬────────────┬────────────┬────────────┬───┬─────────┬─────────┬────────┬────────────┐\n",
       "│ movieId_idx ┆ genres_idx ┆ production ┆ production ┆ … ┆ revenue ┆ runtime ┆ budget ┆ popularity │\n",
       "│ ---         ┆ ---        ┆ _companies ┆ _countries ┆   ┆ ---     ┆ ---     ┆ ---    ┆ ---        │\n",
       "│ i64         ┆ list[i64]  ┆ _idx       ┆ _idx       ┆   ┆ f64     ┆ i64     ┆ f64    ┆ f64        │\n",
       "│             ┆            ┆ ---        ┆ ---        ┆   ┆         ┆         ┆        ┆            │\n",
       "│             ┆            ┆ list[i64]  ┆ list[i64]  ┆   ┆         ┆         ┆        ┆            │\n",
       "╞═════════════╪════════════╪════════════╪════════════╪═══╪═════════╪═════════╪════════╪════════════╡\n",
       "│ 5574        ┆ [11, 14]   ┆ [8939,     ┆ [64, 71]   ┆ … ┆ 0.0     ┆ 124     ┆ 0.0    ┆ 2.796      │\n",
       "│             ┆            ┆ 33821]     ┆            ┆   ┆         ┆         ┆        ┆            │\n",
       "└─────────────┴────────────┴────────────┴────────────┴───┴─────────┴─────────┴────────┴────────────┘"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# movies_df['popularity'].sort(descending=True)\n",
    "movies_df.filter(pl.col('movieId_idx') == 5574)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "32377f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pl.read_csv('../data/ml-32m/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "9881d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_popular_negatives(positive_ids: torch.Tensor, n_movies: int, num_negatives: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Samples popular negative movie indices for each positive ID in the batch.\n",
    "    Ensures that negative samples do not include the corresponding positive ID.\n",
    "\n",
    "    Args:\n",
    "        positive_ids (torch.Tensor): Tensor of shape [batch_size] with positive movie indices.\n",
    "        n_movies (int): Total number of movies in the vocabulary.\n",
    "        num_negatives (int): Number of negative samples per positive.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape [batch_size, num_negatives] with popular negative movie indices.\n",
    "    \"\"\"\n",
    "    batch_size = positive_ids.size(0)\n",
    "    num_negatives = min(num_negatives, n_movies - 1)  # Ensure we don't ask for more than possible\n",
    "\n",
    "    # Create tensor of all possible movie IDs\n",
    "    all_movie_ids = torch.arange(n_movies, device=positive_ids.device)\n",
    "    # Create a tensor of popular movie IDs using poplularity column from movies_df (e.g., top 1000)\n",
    "    popular_movie_ids = ratings_df.group_by('movieId').agg(pl.count('rating').alias('count')).sort('count', descending=True).head(1000)['movieId'].to_list()\n",
    "    popular_movie_ids = torch.tensor([movie_vocab_stoi.get(f'movie_{mid}', 0) for mid in popular_movie_ids], device=positive_ids.device)\n",
    "\n",
    "    # Allocate tensor for results\n",
    "    final_negative_ids = torch.empty(batch_size, num_negatives, dtype=torch.long, device=positive_ids.device)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        pos_id = positive_ids[i]  # No .item(), stays on GPU\n",
    "        # Exclude the positive ID from possible negatives\n",
    "        mask = popular_movie_ids != pos_id\n",
    "        possible_negatives = all_movie_ids[mask]\n",
    "\n",
    "        if possible_negatives.numel() < num_negatives:\n",
    "            # Not enough negatives — warn and pad with zeros\n",
    "            print(f\"Warning: Not enough negatives for index {i}, only {possible_negatives.numel()} available.\")\n",
    "            sampled = torch.randperm(possible_negatives.numel(), device=positive_ids.device)\n",
    "            final_negative_ids[i, :possible_negatives.numel()] = possible_negatives[sampled]\n",
    "            final_negative_ids[i, possible_negatives.numel():] = 0  # Assumes 0 is a valid ID or placeholder\n",
    "        else:\n",
    "            # Sample without replacement from possible_negatives\n",
    "            sampled = torch.randperm(possible_negatives.numel(), device=positive_ids.device)[:num_negatives]\n",
    "            final_negative_ids[i] = possible_negatives[sampled]\n",
    "\n",
    "    return final_negative_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "0d0235f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 11]) torch.Size([256]) torch.Size([256, 11]) torch.Size([256])\n",
      "tensor([70625, 54458, 63325,  6915, 50795, 66773, 39192, 29484, 19173, 76621,\n",
      "        69062], device='cuda:0')\n",
      "torch.Size([256, 86494])\n"
     ]
    }
   ],
   "source": [
    "model = TRXModel(\n",
    "    d_model=8,\n",
    "    n_heads=4,\n",
    "    num_layers=4,\n",
    "    hidden_size=64,\n",
    "    num_list_features=7,\n",
    "    num_scalar_features=7,\n",
    "    n_genres=len_genres, \n",
    "    n_production_companies=len_prod_comp, \n",
    "    n_production_countries=len_prod_cont, \n",
    "    n_spoken_languages=len_langs, \n",
    "    movie_vocab_stoi=movie_vocab_stoi,\n",
    "    user_vocab_stoi=user_vocab_stoi,\n",
    "    n_movies=len(movie_vocab_stoi),\n",
    "    n_words=len_words,\n",
    "    fc_size=32\n",
    ").to('cuda')\n",
    "\n",
    "for idx, (movie_data, user_data, ratings_data, y_train) in enumerate(train_iter):\n",
    "    print(movie_data.shape, user_data.shape, ratings_data.shape, y_train.shape)\n",
    "    print(movie_data[0])\n",
    "    y = model(movie_data, movies_df)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "3c8a2add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 13.03M\n"
     ]
    }
   ],
   "source": [
    "# Total parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters in the model: {total_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abb907",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "0dc9b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topk_movies(model: TRXModel,\n",
    "                       sequence_movie_ids: torch.Tensor,\n",
    "                       movies_df: pl.DataFrame,\n",
    "                       movie_vocab_itos: Dict[int, str],\n",
    "                       top_k: int = 10,\n",
    "                       device: torch.device = torch.device('cuda')) -> List[List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    Predicts the top-k next movies for a batch of input sequences.\n",
    "\n",
    "    Args:\n",
    "        model (TRXModel): The trained TRXModel.\n",
    "        sequence_movie_ids (torch.Tensor): Input tensor of shape [batch_size, seq_len]\n",
    "                                           containing movie indices for sequences.\n",
    "        movies_df (pl.DataFrame): The Polars DataFrame containing movie features.\n",
    "                                  Required by the model's forward pass.\n",
    "        movie_vocab_itos (Dict[int, str]): Dictionary mapping movie index to original ID/string.\n",
    "        top_k (int): The number of top recommendations to return for each sequence.\n",
    "        device (torch.device): The device the model is on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        List[List[Tuple[str, float]]]: A list of lists. Each inner list contains\n",
    "                                       (movie_id_string, score) tuples for the\n",
    "                                       top-k recommendations for one input sequence in the batch.\n",
    "    \"\"\"\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    sequence_movie_ids = sequence_movie_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_movie_logits = model(sequence_movie_ids, movies_df) # Shape: [batch_size, n_movies]\n",
    "        topk_scores, topk_indices = torch.topk(all_movie_logits, k=top_k, dim=1) # Both shape: [batch_size, top_k]\n",
    "\n",
    "    # Convert results back to Python lists and map indices to original movie IDs/strings\n",
    "    results = []\n",
    "    topk_indices = topk_indices.cpu().tolist()\n",
    "    topk_scores = topk_scores.cpu().tolist()\n",
    "\n",
    "    for i in range(len(topk_indices)):\n",
    "        sequence_results = []\n",
    "        for j in range(top_k):\n",
    "            movie_index = topk_indices[i][j]\n",
    "            score = topk_scores[i][j]\n",
    "            # Map the index back to the original movie ID string\n",
    "            movie_id_string = movie_vocab_itos[movie_index] # Handle unknown indices\n",
    "\n",
    "            sequence_results.append((movie_id_string, score))\n",
    "        results.append(sequence_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "b585006f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie_64614'"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_vocab_itos[44916]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "69f2063d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for sequence 1:\n",
      "  Movie ID: movie_1210, Score: 7.0559\n",
      "  Movie ID: movie_2028, Score: 6.8235\n",
      "  Movie ID: movie_40815, Score: 6.5902\n",
      "  Movie ID: movie_4886, Score: 6.4509\n",
      "  Movie ID: movie_1291, Score: 6.3563\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "dummy_sequences_idx = torch.tensor([\n",
    "    [44916, 27309,  1330, 34067, 10537, 84627, 43299, 6341, 52722, 1217, 75094]\n",
    "], dtype=torch.long) # Shape [1, 5]\n",
    "\n",
    "top_k = 5\n",
    "movie_vocab_itos = movie_vocab.get_itos()\n",
    "recommendations = predict_topk_movies(model, dummy_sequences_idx, movies_df, movie_vocab_itos, top_k, device='cuda')\n",
    "\n",
    "# Print the recommendations\n",
    "for i, recs in enumerate(recommendations):\n",
    "    print(f\"Recommendations for sequence {i+1}:\")\n",
    "    for movie_id, score in recs:\n",
    "        print(f\"  Movie ID: {movie_id}, Score: {score:.4f}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "2cc73b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "NUM_NEGATIVES = 20\n",
    "\n",
    "model = TRXModel(\n",
    "    d_model=8,\n",
    "    n_heads=4,\n",
    "    num_layers=4,\n",
    "    hidden_size=64,\n",
    "    num_list_features=7,\n",
    "    num_scalar_features=7,\n",
    "    n_genres=len_genres, \n",
    "    n_production_companies=len_prod_comp, \n",
    "    n_production_countries=len_prod_cont, \n",
    "    n_spoken_languages=len_langs, \n",
    "    movie_vocab_stoi=movie_vocab_stoi,\n",
    "    user_vocab_stoi=user_vocab_stoi,\n",
    "    n_movies=len(movie_vocab_stoi),\n",
    "    n_words=len_words,\n",
    "    fc_size=32\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "aef64857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    for idx, (sequence_movie_ids, user_data, ratings_data, target_movie_id) in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sequence_movie_ids = sequence_movie_ids.to('cuda')\n",
    "        target_movie_id = target_movie_id.to('cuda') # This is the positive target index\n",
    "\n",
    "        all_movie_logits = model(sequence_movie_ids, movies_df) # Shape: [batch_size, n_movies]\n",
    "        negative_movie_ids = sample_negatives(target_movie_id, model.n_movies, NUM_NEGATIVES) # Shape: [batch_size, NUM_NEGATIVES]\n",
    "\n",
    "        positive_logits = all_movie_logits[torch.arange(all_movie_logits.size(0), device=all_movie_logits.device), target_movie_id] # Shape: [batch_size]\n",
    "        negative_logits = torch.gather(all_movie_logits, 1, negative_movie_ids) # Shape: [batch_size, NUM_NEGATIVES]\n",
    "        sampled_logits = torch.cat([positive_logits.unsqueeze(1), negative_logits], dim=1)\n",
    "\n",
    "        targets = torch.zeros(sampled_logits.size(0), dtype=torch.long, device=sampled_logits.device)\n",
    "        loss = criterion(sampled_logits, targets)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 50 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {idx}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "e8305490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_iter) -> float:\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (sequence_movie_ids, user_data, ratings_data, target_movie_id) in tqdm(enumerate(eval_iter)):\n",
    "            sequence_movie_ids = sequence_movie_ids.to('cuda')\n",
    "            target_movie_id = target_movie_id.to('cuda')\n",
    "\n",
    "            all_movie_logits = model(sequence_movie_ids, movies_df)  # [batch_size, n_movies]\n",
    "\n",
    "            negative_movie_ids = sample_negatives(target_movie_id, model.n_movies, NUM_NEGATIVES)  # [batch_size, num_negatives]\n",
    "            positive_logits = all_movie_logits[torch.arange(all_movie_logits.size(0), device=all_movie_logits.device), target_movie_id]  # [batch_size]\n",
    "            negative_logits = torch.gather(all_movie_logits, 1, negative_movie_ids)  # [batch_size, num_negatives]\n",
    "\n",
    "            sampled_logits = torch.cat([positive_logits.unsqueeze(1), negative_logits], dim=1)  # [batch_size, 1 + num_negatives]\n",
    "            targets = torch.zeros(sampled_logits.size(0), dtype=torch.long, device=sampled_logits.device)  # [batch_size]\n",
    "\n",
    "            loss = criterion(sampled_logits, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(eval_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 3.043203830718994\n",
      "Epoch 1, Batch 50, Loss: 1.4328895807266235\n",
      "Epoch 1, Batch 100, Loss: 1.064068078994751\n",
      "Epoch 1, Batch 150, Loss: 1.2194185256958008\n",
      "Epoch 1, Batch 200, Loss: 0.8671781420707703\n",
      "Epoch 1, Batch 250, Loss: 1.068665862083435\n",
      "Epoch 1, Batch 300, Loss: 0.8908101916313171\n",
      "Epoch 1, Batch 350, Loss: 0.9320752024650574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:02,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 665.79s | valid loss  0.86 | valid ppl     2.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 2, Batch 0, Loss: 0.7333865761756897\n",
      "Epoch 2, Batch 50, Loss: 0.8247813582420349\n",
      "Epoch 2, Batch 100, Loss: 0.8839601874351501\n",
      "Epoch 2, Batch 150, Loss: 0.8072408437728882\n",
      "Epoch 2, Batch 200, Loss: 0.8296509385108948\n",
      "Epoch 2, Batch 250, Loss: 0.9697451591491699\n",
      "Epoch 2, Batch 300, Loss: 0.8069130182266235\n",
      "Epoch 2, Batch 350, Loss: 0.816568911075592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:02,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 661.07s | valid loss  0.86 | valid ppl     2.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch 3, Batch 0, Loss: 0.717278003692627\n",
      "Epoch 3, Batch 50, Loss: 0.7670249342918396\n",
      "Epoch 3, Batch 100, Loss: 0.7033736109733582\n",
      "Epoch 3, Batch 150, Loss: 0.6976521611213684\n",
      "Epoch 3, Batch 200, Loss: 0.9144035577774048\n",
      "Epoch 3, Batch 250, Loss: 0.7488551139831543\n",
      "Epoch 3, Batch 300, Loss: 0.7744168043136597\n",
      "Epoch 3, Batch 350, Loss: 0.7861409187316895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:02,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 652.50s | valid loss  0.86 | valid ppl     2.36\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\arnav\\\\AppData\\\\Local\\\\Temp\\\\tmp7d7fcg07\\\\best_model_params.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[342], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), best_model_params_path)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_params_path\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# load best model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arnav\\Documents\\VSCodeProjects\\RecSysTRX\\RecSysTRX\\.venv\\lib\\site-packages\\torch\\serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\arnav\\Documents\\VSCodeProjects\\RecSysTRX\\RecSysTRX\\.venv\\lib\\site-packages\\torch\\serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\arnav\\Documents\\VSCodeProjects\\RecSysTRX\\RecSysTRX\\.venv\\lib\\site-packages\\torch\\serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\arnav\\\\AppData\\\\Local\\\\Temp\\\\tmp7d7fcg07\\\\best_model_params.pt'"
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Training\n",
    "        train(epoch)\n",
    "\n",
    "        # Evaluation\n",
    "        val_loss = evaluate(model, val_iter)\n",
    "\n",
    "        # Compute the perplexity of the validation loss\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "\n",
    "        # Results\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        # scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "a2979a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26812"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_vocab_stoi = movie_vocab.get_stoi()\n",
    "movie_vocab['movie_122912']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "c5c7f4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('movie_47', 5.109602451324463),\n",
       "  ('movie_589', 4.820800304412842),\n",
       "  ('movie_59784', 4.704018592834473),\n",
       "  ('<unk>', 4.67459774017334),\n",
       "  ('movie_60069', 4.658385753631592)]]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_topk_movies(model,\n",
    "                    torch.tensor([43299, 6341,  78663, 1217, 40518, 1376, 48320, 11031, 84283, 51781, 26812], device='cuda').unsqueeze(0), \n",
    "                    movies_df, \n",
    "                    movie_vocab_itos, \n",
    "                    top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763570c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6c4de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
