{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6f6f4a",
   "metadata": {},
   "source": [
    "# Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e39adef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from tempfile import TemporaryDirectory\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8ddfb",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58200b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arnav\\Documents\\VSCodeProjects\\RecSysTRX\\RecSysTRX\\.venv\\lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\arnav\\Documents\\VSCodeProjects\\RecSysTRX\\RecSysTRX\\.venv\\lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "movies_df = pl.read_parquet('../data/processed/output.parquet')\n",
    "train_df = pl.read_parquet('../data/processed/train.parquet')\n",
    "test_df = pl.read_parquet('../data/processed/test.parquet')\n",
    "vocabs = torch.load('../data/processed/all_vocabs.pth')\n",
    "\n",
    "user_vocab = vocabs[\"user_vocab\"]\n",
    "movie_vocab = vocabs[\"movie_vocab\"]\n",
    "genres_vocab = vocabs[\"genres_vocab\"]\n",
    "prod_comp_vocab = vocabs[\"prod_comp_vocab\"]\n",
    "prod_countries_vocab = vocabs[\"prod_countries_vocab\"]\n",
    "languages_vocab = vocabs[\"languages_vocab\"]\n",
    "words_vocab = vocabs[\"words_vocab\"]\n",
    "\n",
    "vocabs = {\n",
    "    \"user_vocab\": user_vocab,\n",
    "    \"movie_vocab\": movie_vocab,\n",
    "    \"genres_vocab\": genres_vocab,\n",
    "    \"prod_comp_vocab\": prod_comp_vocab,\n",
    "    \"prod_countries_vocab\": prod_countries_vocab,\n",
    "    \"languages_vocab\": languages_vocab,\n",
    "    \"words_vocab\": words_vocab,\n",
    "}\n",
    "\n",
    "movie_vocab_stoi = movie_vocab.get_stoi()\n",
    "user_vocab_stoi = user_vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9460c261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies DF shape: (86493, 15)\n",
      "Train DF shape: (100000, 3)\n",
      "Test DF shape: (1000, 3)\n",
      "Vocab sizes: {'user_vocab': 200949, 'movie_vocab': 86494, 'genres_vocab': 21, 'prod_comp_vocab': 45546, 'prod_countries_vocab': 201, 'languages_vocab': 164, 'words_vocab': 270246}\n",
      "\n",
      "First movie row: {'movieId_idx': [29614], 'genres_idx': [[9, 12, 1]], 'production_companies_idx': [[5787, 10790, 33238]], 'production_countries_idx': [[199, 179]], 'spoken_languages_idx': [[6, 23, 137, 47]], 'keywords_idx': [[16308, 175809, 93150, 146205, 199438, 206752, 33269, 147659, 198442, 112495, 215262, 104288, 10122, 84395, 101318, 37337, 35868, 263713, 52720, 82866]], 'overview_idx': [[176485, 24219, 129031, 130595, 204009, 102295, 187482, 46191, 265111, 75891, 120691, 82866, 7607, 80603, 210357, 60107, 248412, 24219, 171073, 185742, 207323, 80603, 208294, 186120, 230514, 250658, 48946, 24219, 162914, 55906, 185742, 145939, 86280, 230898, 120691, 50712, 7607, 233356, 69050, 257026, 247940, 24219, 99177, 12348]], 'tagline_idx': [[214081, 63727, 60107, 120691, 122910, 7607, 120691, 55301]], 'adult_idx': [0], 'vote_average': [8.364], 'vote_count': [34495], 'revenue': [8255.327640000001], 'runtime': [148], 'budget': [1600.0000000000002], 'popularity': [83.952]}\n",
      "First sequence row: {'userId': ['user_20007'], 'sequence_movie_ids': [['movie_64983', 'movie_106916', 'movie_4452', 'movie_41573', 'movie_108932', 'movie_36527', 'movie_79185', 'movie_70599', 'movie_46967', 'movie_43560', 'movie_102903', 'movie_68793']], 'sequence_ratings': [[2.5, 4.0, 3.0, 3.5, 3.0, 4.0, 2.5, 3.0, 4.0, 3.5, 3.0, 4.5]]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Movies DF shape: {movies_df.shape}\")\n",
    "print(f\"Train DF shape: {train_df.shape}\")\n",
    "print(f\"Test DF shape: {test_df.shape}\")\n",
    "print(\"Vocab sizes:\", {k: len(v) for k, v in vocabs.items()})\n",
    "print(\"\\nFirst movie row:\", movies_df[0].to_dict(as_series=False))\n",
    "print(\"First sequence row:\", train_df[0].to_dict(as_series=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55ad7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get Vocab Sizes for Model ---\n",
    "len_genres = len(genres_vocab)\n",
    "len_prod_comp = len(prod_comp_vocab)\n",
    "len_prod_cont = len(prod_countries_vocab)\n",
    "len_langs = len(languages_vocab)\n",
    "len_words = len(words_vocab)\n",
    "n_items = len(movie_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d32cc059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('userId', String),\n",
       "        ('sequence_movie_ids', List(String)),\n",
       "        ('sequence_ratings', List(Float64))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "180a2eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class MovieSeqDataset(Dataset):\n",
    "    def __init__(self, data, movie_vocab_stoi, user_vocab_stoi):\n",
    "        self.data = data\n",
    "        self.movie_vocab_stoi = movie_vocab_stoi\n",
    "        self.user_vocab_stoi = user_vocab_stoi\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user, movie_sequence, rating_sequence = self.data[idx]\n",
    "        movie_data = [self.movie_vocab_stoi.get(item,movie_vocab_stoi['<unk>']) for item in movie_sequence.to_list()[0]]\n",
    "        user_data = self.user_vocab_stoi[user.to_list()[0]]\n",
    "        rating_sequence = rating_sequence.to_list()[0]\n",
    "        return torch.tensor(movie_data[:-1],device='cuda'), torch.tensor(user_data), torch.tensor(rating_sequence[:-1]), torch.tensor(movie_data[-1],device='cuda')\n",
    "\n",
    "def collate_batch(batch):\n",
    "    movie_list = [item[0] for item in batch]\n",
    "    user_list = [item[1] for item in batch]\n",
    "    rating_list = [item[2] for item in batch]\n",
    "    target_list = [item[3] for item in batch]\n",
    "    return pad_sequence(movie_list, padding_value=movie_vocab_stoi['<unk>'], batch_first=True), torch.stack(user_list), pad_sequence(rating_list, padding_value=3, batch_first=True), torch.stack(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a64a911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "train_dataset = MovieSeqDataset(train_df, movie_vocab_stoi, user_vocab_stoi)\n",
    "val_dataset = MovieSeqDataset(test_df, movie_vocab_stoi, user_vocab_stoi)\n",
    "\n",
    "train_iter = DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True, collate_fn=collate_batch)\n",
    "val_iter = DataLoader(val_dataset, batch_size=BATCH_SIZE,shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f928e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 11]) torch.Size([256]) torch.Size([256, 11]) torch.Size([256])\n",
      "tensor([15837, 27579, 24675, 78108, 33476, 78820, 40442,  2674, 21279, 76577,\n",
      "        71546], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i, (movie_data, user_data, ratings_data, y_train) in enumerate(train_iter):\n",
    "    print(movie_data.shape, user_data.shape, ratings_data.shape, y_train.shape)\n",
    "    print(movie_data[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60dbc3a",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d3c03ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from typing import Tuple\n",
    "\n",
    "class MovieEmbeddings(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 hidden_size: int,\n",
    "                 num_list_features: int,\n",
    "                 num_scalar_features: int,\n",
    "                 n_genres: int, \n",
    "                 n_production_companies: int,\n",
    "                 n_production_countries: int,\n",
    "                 n_spoken_languages: int,\n",
    "                 n_words: int):\n",
    "        super().__init__()\n",
    "        self.genres_embedding = nn.EmbeddingBag(n_genres, d_model*2, mode='mean')\n",
    "        self.prod_comp_embedding = nn.EmbeddingBag(n_production_companies, d_model, mode='mean')\n",
    "        self.prod_cont_embedding = nn.EmbeddingBag(n_production_countries, d_model, mode='mean')\n",
    "        self.lang_embedding = nn.EmbeddingBag(n_spoken_languages, d_model, mode='mean')\n",
    "        self.word_embedding = nn.EmbeddingBag(n_words, d_model*4, mode='mean')\n",
    "        self.fc = nn.Linear(d_model*(10+num_list_features)+num_scalar_features,hidden_size)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.xavier_uniform_(self.genres_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.prod_comp_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.prod_cont_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.lang_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.word_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def _prepare_embedding_inputs(self, list_of_lists) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        flat_list = []\n",
    "        offsets = [0]\n",
    "        for sublist in list_of_lists:\n",
    "            flat_list.extend(sublist)\n",
    "            offsets.append(offsets[-1] + len(sublist))\n",
    "        offsets = offsets[:-1]  # Remove last cumulative sum\n",
    "        offsets = torch.tensor(offsets, dtype=torch.long,device='cuda')\n",
    "        flat_list = torch.tensor(flat_list, dtype=torch.long,device='cuda')\n",
    "        return flat_list, offsets   \n",
    "\n",
    "    def forward(self, row: pl.DataFrame) -> torch.Tensor:\n",
    "        genres, genres_offsets = self._prepare_embedding_inputs(row['genres_idx'])\n",
    "        genres_e = self.genres_embedding(genres, genres_offsets)\n",
    "\n",
    "        comp, comp_offsets = self._prepare_embedding_inputs(row['production_companies_idx'])\n",
    "        comp_e = self.prod_comp_embedding(comp, comp_offsets)\n",
    "\n",
    "        cont, cont_offsets = self._prepare_embedding_inputs(row['production_countries_idx'])\n",
    "        cont_e = self.prod_cont_embedding(cont, cont_offsets)\n",
    "\n",
    "        lang, lang_offsets = self._prepare_embedding_inputs(row['spoken_languages_idx'])\n",
    "        lang_e = self.lang_embedding(lang, lang_offsets)\n",
    "\n",
    "        kw, kw_offsets = self._prepare_embedding_inputs(row['keywords_idx'])\n",
    "        kw_e = self.word_embedding(kw, kw_offsets)\n",
    "\n",
    "        tag, tag_offsets = self._prepare_embedding_inputs(row['tagline_idx'])\n",
    "        tag_e = self.word_embedding(tag, tag_offsets)\n",
    "\n",
    "        ov, ov_offsets = self._prepare_embedding_inputs(row['overview_idx'])\n",
    "        ov_e = self.word_embedding(ov, ov_offsets)\n",
    "\n",
    "        # Scalar features as tensors (ensure shape is [batch_size, 1])\n",
    "        revenue = torch.tensor(row[\"revenue\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "        budget = torch.tensor(row[\"budget\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "        runtime = torch.tensor(row[\"runtime\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "        adult_idx = torch.tensor(row[\"adult_idx\"], dtype=torch.bool,device='cuda').unsqueeze(1)\n",
    "        vote_average = torch.tensor(row[\"vote_average\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "        vote_count = torch.tensor(row[\"vote_count\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "        popularity = torch.tensor(row[\"popularity\"], dtype=torch.float32,device='cuda').unsqueeze(1)\n",
    "\n",
    "        # Concatenate all embeddings and scalar features\n",
    "        master_embedding = torch.cat([\n",
    "            genres_e,\n",
    "            comp_e,\n",
    "            cont_e,\n",
    "            lang_e,\n",
    "            kw_e,\n",
    "            tag_e,\n",
    "            ov_e,\n",
    "            revenue,\n",
    "            budget,\n",
    "            runtime,\n",
    "            adult_idx,\n",
    "            vote_average,\n",
    "            vote_count,\n",
    "            popularity\n",
    "        ], dim=1)\n",
    "\n",
    "        return self.fc(master_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df8d30f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e06f6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRXTransformer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, batch_first=True) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x)\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "c6532702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRXModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 n_heads: int,\n",
    "                 num_layers: int,\n",
    "                 hidden_size: int, # This is the output dimension of MovieEmbeddings' FC layer\n",
    "                 num_list_features: int,\n",
    "                 num_scalar_features: int,\n",
    "                 n_genres: int,\n",
    "                 n_production_companies: int,\n",
    "                 n_production_countries: int,\n",
    "                 n_spoken_languages: int,\n",
    "                 movie_vocab_stoi: Dict[str, int],\n",
    "                 user_vocab_stoi: Dict[str, int],\n",
    "                 n_movies: int,\n",
    "                 n_words: int,\n",
    "                 fc_size: int = 512,\n",
    "                 seq_len: int = 11,\n",
    "                 dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.num_list_features = num_list_features\n",
    "        self.num_scalar_features = num_scalar_features\n",
    "        self.n_genres = n_genres\n",
    "        self.n_production_companies = n_production_companies\n",
    "        self.n_production_countries = n_production_countries\n",
    "        self.n_spoken_languages = n_spoken_languages\n",
    "        self.n_movies = n_movies\n",
    "        self.n_words = n_words\n",
    "        self.fc_size = fc_size\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.movie_embeddings = MovieEmbeddings(d_model, hidden_size, num_list_features, num_scalar_features, n_genres, n_production_companies, n_production_countries, n_spoken_languages, n_words)\n",
    "        self.movie_vocab_stoi = movie_vocab_stoi\n",
    "        self.user_vocab_stoi = user_vocab_stoi\n",
    "\n",
    "        self.transformer_encoder = TRXTransformer(hidden_size, n_heads=n_heads, num_layers=num_layers)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size*seq_len, fc_size)\n",
    "        self.fc2 = nn.Linear(fc_size, n_movies)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self._init_weights()\n",
    "\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, movies_df: pl.DataFrame) -> torch.Tensor:\n",
    "        # x: [batch_size, seq_len]\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        sequence_movie_rows: List[pl.DataFrame] = [movies_df.filter(pl.col('movieId_idx').is_in(x[i].cpu().tolist())) for i in range(batch_size)]\n",
    "        embeddings = [self.movie_embeddings(row) for row in sequence_movie_rows] # List of tensors [seq_len, hidden_size]\n",
    "\n",
    "        # Stack and pad embeddings if sequences are different sizes\n",
    "        max_len = max([emb.shape[0] for emb in embeddings]) if embeddings else 0\n",
    "\n",
    "        # Handle empty batch or empty sequences gracefully\n",
    "        if max_len == 0:\n",
    "            return torch.zeros(batch_size, self.fc2.out_features, device=x.device)\n",
    "\n",
    "        padded_embeddings = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            current_seq_len = embeddings[i].shape[0]\n",
    "            if current_seq_len < max_len:\n",
    "                # Pad with zeros along the sequence dimension - NOW self.hidden_size is available\n",
    "                pad_tensor = torch.zeros(max_len - current_seq_len, self.hidden_size, device='cuda')\n",
    "                padded_embeddings.append(torch.cat([embeddings[i], pad_tensor], dim=0))\n",
    "            else:\n",
    "                padded_embeddings.append(embeddings[i][:max_len]) # Truncate if somehow longer\n",
    "\n",
    "        embeddings = torch.stack(padded_embeddings, dim=0)\n",
    "        # embeddings shape: [batch_size, max_len, hidden_size]\n",
    "\n",
    "        # Pass Embeddings through the Transformer Encoder\n",
    "        transformer_output = self.transformer_encoder(embeddings)\n",
    "        # transformer_output shape: [batch_size, max_len, hidden_size]\n",
    "\n",
    "        # Reshape transformer_output to feed into fc1\n",
    "        transformer_output = transformer_output.view(transformer_output.size(0), -1)\n",
    "        # transformer_output shape: [batch_size, max_len * hidden_size]\n",
    "\n",
    "        # Pass to fc1\n",
    "        fc1_output = self.fc1(transformer_output)\n",
    "        # fc1_output shape: [batch_size, fc_size]\n",
    "\n",
    "        # Apply dropout\n",
    "        fc1_output = self.dropout(fc1_output)\n",
    "\n",
    "        # Pass to fc2 (outputs scores for all n_movies)\n",
    "        fc2_output = self.fc2(fc1_output)\n",
    "        # fc2_output shape: [batch_size, n_movies]\n",
    "\n",
    "        return fc2_output # Return raw logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "88af6a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_negatives(positive_ids: torch.Tensor, n_movies: int, num_negatives: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Samples negative movie indices for each positive ID in the batch.\n",
    "    Ensures that negative samples do not include the corresponding positive ID.\n",
    "\n",
    "    Args:\n",
    "        positive_ids (torch.Tensor): Tensor of shape [batch_size] with positive movie indices.\n",
    "        n_movies (int): Total number of movies in the vocabulary.\n",
    "        num_negatives (int): Number of negative samples per positive.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape [batch_size, num_negatives] with negative movie indices.\n",
    "    \"\"\"\n",
    "    batch_size = positive_ids.size(0)\n",
    "    num_negatives = min(num_negatives, n_movies - 1)  # Ensure we don't ask for more than possible\n",
    "\n",
    "    # Create tensor of all possible movie IDs\n",
    "    all_movie_ids = torch.arange(n_movies, device=positive_ids.device)\n",
    "\n",
    "    # Allocate tensor for results\n",
    "    final_negative_ids = torch.empty(batch_size, num_negatives, dtype=torch.long, device=positive_ids.device)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        pos_id = positive_ids[i]  # No .item(), stays on GPU\n",
    "        # Exclude the positive ID from possible negatives\n",
    "        mask = all_movie_ids != pos_id\n",
    "        possible_negatives = all_movie_ids[mask]\n",
    "\n",
    "        if possible_negatives.numel() < num_negatives:\n",
    "            # Not enough negatives — warn and pad with zeros\n",
    "            print(f\"Warning: Not enough negatives for index {i}, only {possible_negatives.numel()} available.\")\n",
    "            sampled = torch.randperm(possible_negatives.numel(), device=positive_ids.device)\n",
    "            final_negative_ids[i, :possible_negatives.numel()] = possible_negatives[sampled]\n",
    "            final_negative_ids[i, possible_negatives.numel():] = 0  # Assumes 0 is a valid ID or placeholder\n",
    "        else:\n",
    "            # Sample without replacement from possible_negatives\n",
    "            sampled = torch.randperm(possible_negatives.numel(), device=positive_ids.device)[:num_negatives]\n",
    "            final_negative_ids[i] = possible_negatives[sampled]\n",
    "\n",
    "    return final_negative_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "0d0235f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 11]) torch.Size([256]) torch.Size([256, 11]) torch.Size([256])\n",
      "tensor([72489, 76417, 50429, 43890, 68666, 85271, 41492, 12780, 65928, 26366,\n",
      "        54896], device='cuda:0')\n",
      "torch.Size([256, 86494])\n"
     ]
    }
   ],
   "source": [
    "model = TRXModel(\n",
    "    d_model=8,\n",
    "    n_heads=4,\n",
    "    num_layers=4,\n",
    "    hidden_size=64,\n",
    "    num_list_features=7,\n",
    "    num_scalar_features=7,\n",
    "    n_genres=len_genres, \n",
    "    n_production_companies=len_prod_comp, \n",
    "    n_production_countries=len_prod_cont, \n",
    "    n_spoken_languages=len_langs, \n",
    "    movie_vocab_stoi=movie_vocab_stoi,\n",
    "    user_vocab_stoi=user_vocab_stoi,\n",
    "    n_movies=len(movie_vocab_stoi),\n",
    "    n_words=len_words,\n",
    "    fc_size=32\n",
    ").to('cuda')\n",
    "\n",
    "for idx, (movie_data, user_data, ratings_data, y_train) in enumerate(train_iter):\n",
    "    print(movie_data.shape, user_data.shape, ratings_data.shape, y_train.shape)\n",
    "    print(movie_data[0])\n",
    "    y = model(movie_data, movies_df)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "3c8a2add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 13.03M\n"
     ]
    }
   ],
   "source": [
    "# Total parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters in the model: {total_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abb907",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "0dc9b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topk_movies(model: TRXModel,\n",
    "                       sequence_movie_ids: torch.Tensor,\n",
    "                       movies_df: pl.DataFrame,\n",
    "                       movie_vocab_itos: Dict[int, str],\n",
    "                       top_k: int = 10,\n",
    "                       device: torch.device = torch.device('cuda')) -> List[List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    Predicts the top-k next movies for a batch of input sequences.\n",
    "\n",
    "    Args:\n",
    "        model (TRXModel): The trained TRXModel.\n",
    "        sequence_movie_ids (torch.Tensor): Input tensor of shape [batch_size, seq_len]\n",
    "                                           containing movie indices for sequences.\n",
    "        movies_df (pl.DataFrame): The Polars DataFrame containing movie features.\n",
    "                                  Required by the model's forward pass.\n",
    "        movie_vocab_itos (Dict[int, str]): Dictionary mapping movie index to original ID/string.\n",
    "        top_k (int): The number of top recommendations to return for each sequence.\n",
    "        device (torch.device): The device the model is on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        List[List[Tuple[str, float]]]: A list of lists. Each inner list contains\n",
    "                                       (movie_id_string, score) tuples for the\n",
    "                                       top-k recommendations for one input sequence in the batch.\n",
    "    \"\"\"\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    sequence_movie_ids = sequence_movie_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_movie_logits = model(sequence_movie_ids, movies_df) # Shape: [batch_size, n_movies]\n",
    "        topk_scores, topk_indices = torch.topk(all_movie_logits, k=top_k, dim=1) # Both shape: [batch_size, top_k]\n",
    "\n",
    "    # Convert results back to Python lists and map indices to original movie IDs/strings\n",
    "    results = []\n",
    "    topk_indices = topk_indices.cpu().tolist()\n",
    "    topk_scores = topk_scores.cpu().tolist()\n",
    "\n",
    "    for i in range(len(topk_indices)):\n",
    "        sequence_results = []\n",
    "        for j in range(top_k):\n",
    "            movie_index = topk_indices[i][j]\n",
    "            score = topk_scores[i][j]\n",
    "            # Map the index back to the original movie ID string\n",
    "            movie_id_string = movie_vocab_itos[movie_index] # Handle unknown indices\n",
    "\n",
    "            sequence_results.append((movie_id_string, score))\n",
    "        results.append(sequence_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b585006f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie_64614'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_vocab_itos[44916]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "69f2063d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for sequence 1:\n",
      "  Movie ID: movie_1210, Score: 7.0559\n",
      "  Movie ID: movie_2028, Score: 6.8235\n",
      "  Movie ID: movie_40815, Score: 6.5902\n",
      "  Movie ID: movie_4886, Score: 6.4509\n",
      "  Movie ID: movie_1291, Score: 6.3563\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "dummy_sequences_idx = torch.tensor([\n",
    "    [44916, 27309,  1330, 34067, 10537, 84627, 43299, 6341, 52722, 1217, 75094]\n",
    "], dtype=torch.long) # Shape [1, 5]\n",
    "\n",
    "top_k = 5\n",
    "movie_vocab_itos = movie_vocab.get_itos()\n",
    "recommendations = predict_topk_movies(model, dummy_sequences_idx, movies_df, movie_vocab_itos, top_k, device='cuda')\n",
    "\n",
    "# Print the recommendations\n",
    "for i, recs in enumerate(recommendations):\n",
    "    print(f\"Recommendations for sequence {i+1}:\")\n",
    "    for movie_id, score in recs:\n",
    "        print(f\"  Movie ID: {movie_id}, Score: {score:.4f}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "2cc73b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRXModel(\n",
       "  (movie_embeddings): MovieEmbeddings(\n",
       "    (genres_embedding): EmbeddingBag(21, 16, mode='mean')\n",
       "    (prod_comp_embedding): EmbeddingBag(45546, 8, mode='mean')\n",
       "    (prod_cont_embedding): EmbeddingBag(201, 8, mode='mean')\n",
       "    (lang_embedding): EmbeddingBag(164, 8, mode='mean')\n",
       "    (word_embedding): EmbeddingBag(270246, 32, mode='mean')\n",
       "    (fc): Linear(in_features=143, out_features=64, bias=True)\n",
       "  )\n",
       "  (transformer_encoder): TRXTransformer(\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=704, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=86494, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "NUM_NEGATIVES = 20\n",
    "\n",
    "model = TRXModel(\n",
    "    d_model=8,\n",
    "    n_heads=4,\n",
    "    num_layers=4,\n",
    "    hidden_size=64,\n",
    "    num_list_features=7,\n",
    "    num_scalar_features=7,\n",
    "    n_genres=len_genres, \n",
    "    n_production_companies=len_prod_comp, \n",
    "    n_production_countries=len_prod_cont, \n",
    "    n_spoken_languages=len_langs, \n",
    "    movie_vocab_stoi=movie_vocab_stoi,\n",
    "    user_vocab_stoi=user_vocab_stoi,\n",
    "    n_movies=len(movie_vocab_stoi),\n",
    "    n_words=len_words,\n",
    "    fc_size=32\n",
    ")\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "520266bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Example optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "aef64857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    for idx, (sequence_movie_ids, user_data, ratings_data, target_movie_id) in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sequence_movie_ids = sequence_movie_ids.to('cuda')\n",
    "        target_movie_id = target_movie_id.to('cuda') # This is the positive target index\n",
    "\n",
    "        all_movie_logits = model(sequence_movie_ids, movies_df) # Shape: [batch_size, n_movies]\n",
    "        negative_movie_ids = sample_negatives(target_movie_id, model.n_movies, NUM_NEGATIVES) # Shape: [batch_size, NUM_NEGATIVES]\n",
    "\n",
    "        positive_logits = all_movie_logits[torch.arange(all_movie_logits.size(0), device=all_movie_logits.device), target_movie_id] # Shape: [batch_size]\n",
    "        negative_logits = torch.gather(all_movie_logits, 1, negative_movie_ids) # Shape: [batch_size, NUM_NEGATIVES]\n",
    "        sampled_logits = torch.cat([positive_logits.unsqueeze(1), negative_logits], dim=1)\n",
    "\n",
    "        targets = torch.zeros(sampled_logits.size(0), dtype=torch.long, device=sampled_logits.device)\n",
    "        loss = criterion(sampled_logits, targets)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 50 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {idx}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "e8305490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_iter) -> float:\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (sequence_movie_ids, user_data, ratings_data, target_movie_id) in tqdm(enumerate(eval_iter)):\n",
    "            sequence_movie_ids = sequence_movie_ids.to('cuda')\n",
    "            target_movie_id = target_movie_id.to('cuda')\n",
    "\n",
    "            all_movie_logits = model(sequence_movie_ids, movies_df)  # [batch_size, n_movies]\n",
    "\n",
    "            negative_movie_ids = sample_negatives(target_movie_id, model.n_movies, NUM_NEGATIVES)  # [batch_size, num_negatives]\n",
    "            positive_logits = all_movie_logits[torch.arange(all_movie_logits.size(0), device=all_movie_logits.device), target_movie_id]  # [batch_size]\n",
    "            negative_logits = torch.gather(all_movie_logits, 1, negative_movie_ids)  # [batch_size, num_negatives]\n",
    "\n",
    "            sampled_logits = torch.cat([positive_logits.unsqueeze(1), negative_logits], dim=1)  # [batch_size, 1 + num_negatives]\n",
    "            targets = torch.zeros(sampled_logits.size(0), dtype=torch.long, device=sampled_logits.device)  # [batch_size]\n",
    "\n",
    "            loss = criterion(sampled_logits, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(eval_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 3.04215931892395\n"
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Training\n",
    "        train(epoch)\n",
    "\n",
    "        # Evaluation\n",
    "        val_loss = evaluate(model, val_iter)\n",
    "\n",
    "        # Compute the perplexity of the validation loss\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "\n",
    "        # Results\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        # scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "a2979a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26812"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_vocab_stoi = movie_vocab.get_stoi()\n",
    "movie_vocab['movie_122912']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "c5c7f4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('movie_47', 7.067640781402588),\n",
       "  ('<unk>', 7.000823020935059),\n",
       "  ('movie_78499', 6.7338995933532715),\n",
       "  ('movie_8368', 6.675086498260498),\n",
       "  ('movie_8961', 6.662047386169434)]]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_topk_movies(model,\n",
    "                    torch.tensor([43299, 6341,  78663, 1217, 40518, 1376, 48320, 11031, 84283, 51781, 26812], device='cuda').unsqueeze(0), \n",
    "                    movies_df, \n",
    "                    movie_vocab_itos, \n",
    "                    top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763570c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
